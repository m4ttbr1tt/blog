[{"content":"Continuing with LFCS prep, we are moving on to user and group management..\nUser management sudo adduser matt # will be prompted for password and info # adds user and group matt # home user is created # default shell is /bin/bash # copied from /etc/skel (like a template) sudo passwd matt sudo deluser matt sudo deluser --remove-home matt sudo adduser --shell /bin/othershell --home /home/otherdir/ matt cat /etc/passwd # for user id home and shell id ls -ln home # will print out user id id # shows the user and group id whoami # shows the current username sudo adduser --system --no-create-home sysacc # system account, intented for progams sudo usermod --home /home/otherdir --move-home matt # move and change home dir sudo usermod -d /home/otherdir -m matt # move and change home dir sudo usermod --login matt bob # change the users name or -l sudo usermod --shell /bin/othershell matt # or -s sudo usermod --lock matt # disables account or -L sudo usermod --unlock matt sudo usermod --expiredate 2028-12-10 matt # account expiry sudo chage --lastday 0 matt # change age of password (will force to login next login) sudo chage --maxdays 30 matt # force change every 30 days sudo chage --maxdays -1 matt # never expires sudo chage --list matt ![[Pasted image 20250708062741.png]]\nLocal Groups and Group Membership # each user can belong to one or more groups # used to manage permissions # user has primary / login group sudo groupadd developers sudo gpasswd --add matt developers # gpasswd is short for group password or -a groups matt # lists all groups that matt is part of sudo gpasswd --delete matt developors # or -d remove from group sudo usermod -g developers matt # sets the primary group sudo usermod --gid matt matt sudo groupmod --new-name programmers developers # rename group name or -n sudo groupdel programmers Manage System-Wide Environment Profiles printenv # or env # user specific env vars .bashrc sudo vim /etc/environment # system wide vars logout echo $SYSVAR # run something everytime any user logs in sudo vim /etc/profile.d/lastlogin.sh # create file Manage Template User Environment sudo vim /etc/skel/README # edit or create new files Configure User Resource Limits sudo /vim/etc/security/limits.conf # domain type item value # domain is user or group or * # type - hard, soft or - # item eg. nproc, fsize, cpu man limits.conf #limits.conf matt - nproc 3 # sudo -iu matt # real login ps | less # shell and these two are three process ls -a | grep bash | less # will not run as limited to 3 processes ulimit -a # see limits ulimit -u 5000 # lower limit or raise up to hard value Manage User Privileges groups # show groups for user # add user to sudo sudo gpasswd -a matt sudo # add user to sudo # edit sudoers file sudo visudo %sudo ALL=(ALL:ALL) ALL #group HOST=(RUNASUSER:RUNASGROUP) LISTOFCOMMANDS matt ALL=(ALL) ALL # no run as group sudo -u john ls /home/john # runs command as user john matt ALL=(john,jane) ALL # matt can only run commands as users john or jane matt ALL=(ALL) /bin/ls, /bin/stat # only certain commands matt ALL= /bin/ls, /bin/stat # only certain commands Manage Access to Root sudo ls /root # runs command as root sudo --login # login as root or -i logout su - # will ask for root passwd su -l # will ask for root passwd su --login # will ask for root passwd sudo passwd --lock root # sets password based logins Configure LDAP user and groups # user accounts are store in /etc/passwd # LDAP server (Lightweight Directory Access Protocol) # Single server change across multiple servers id john # no such user #lxc (linux containers) #lxd init (managers containers) lxc import ldap-server.tar.xz lxc list lxc start ldap-server sudo apt install libnss-ldap cat /etc/nsswitch.conf # name server switch, edited after config (where to find info) eg. files systemd ldap nslcd # name service local daemon (gets the actual data from the ldap server) sudo cat /etc/nslcd.conf # config file #nsl gets the info getent passwd --service ldap getent group --service ldap sudo pam-auth-update # to auto create user home (pluggable auth modules) ","date":"14 July, 2025","id":0,"permalink":"/posts/lfcs-users-and-groups/","summary":"Continuing with LFCS prep, we are moving on to user and group management..","tags":"LFCS Linux Certificates","title":"LFCS - Users and Groups"},{"content":"Understanding VMs and containers, and how to manage them in Linux, is an important skill to master.\nHere is a list of commands that are useful for managing docker containers and QEMU-KVM VMs.\nCreate and Manage Containers docker images # may get a permission denied if not in correct user group docker search nginx docker pull nginx docker pull ubuntu/nginx docker images # list docker rmi ubuntu/nginx # remove images docker run nginx # starts a container, will be inside container docker run --detach --publish 8080:80 --name mycontainer nginx docker ps # lists running containers docker ps --all # all docker start 2342342343 # container id docker stop container_name nc localhost 8080 # connects to port 8080 with netcat docker rm container_name # removes container # order of cleanup docker stop mycontainer docker rm mycontainer docker rmi nginx # container to always restart docker run --detach --publish 8080:80 --name mywebserver nginx --restart always # create an image with Dockerfile FROM nginx COPY index.html /user/share/nginx/html/index.html docker build --tag mattb/customnginx:1.0 mydir Manage Virtual Machines # QEMU-KVM # Quick Emulator - Kernel-based Virtual Machine # virsh - manage virtual machines from the command line sudo apt install virt-manager # installs other cli utils # create an xml config (see online) virsh define testmachine.xml virsh help virsh list --all virsh start TestMachine virsh list virsh reboot TestMachine virsh reset TestMachine # force reset virsh shutdown TestMachine # graceful shutdown virsh destroy TestMachine # hard power off (does not destroy) virsh undefine TestMachine # will remove machine defined virsh autostart TestMachine # automstart when server boots virsh dominfo TestMachine # shows specs virsh set # tab tab # set cpus virsh help setvcpus virsh setvcpus TestMachine 2 --config virsh setvcpus TestMachine 2 --config --maximum # set memory virsh setmaxmem TestMachine 2048M --config virsh setmem TestMachine 2048M --config # download ubuntu image wget https://images......./release/ubuntu.img qemu-img info ubuntu.img qemu-img resize ubuntu.img 10G # changes the size of the virtual disk # add to default storage pools sudo cp ubuntu.img /var/lib/libvirt/images/ virt-install virt-install --osinfo list # lists types of os\u0026#39;s man virt-install # disk images has os preinstalled so we can just import not install! virt-install --help virt-install --osinfo ubuntu24.04 --name ubuntu1 --memory 3072 --vcpus 1 --import --disk /var/lib/libvirt/images/ubuntu.img --graphics none sudo apt install -y libguestfs-tools # setting root password virsh shutdown ubuntu1 sudo virt-customize -a /vir/lib/libvirt/images/ubuntu.img --root-password password:somepassword123 virsh start ubuntu1 virsh console ubuntu1 # reattach to console sudo apt install libosinfo-bin # check for latest os support osinfo-query os # what os are available for the tool # install a vm (without having a cloud image file) rather from an iso virt-install --osinfo debian12 --name debian1 --momory 1024 --vcpus 1 --disk size=10 --location /var/lib/libvirt/boot/debian.iso --graphics none --extra-args \u0026#34;console=ttyS0\u0026#34; # serial port # or you can use a url instead of iso virt-install --osinfo debian12 --name debian1 --momory 1024 --vcpus 1 --disk size=10 --location https://deb.debian.org/debian/dists/bookworm/main/installer-amd64/ --graphics none --extra-args \u0026#34;console=ttyS0\u0026#34; # serial port ","date":"12 July, 2025","id":1,"permalink":"/posts/lfcs-operations-deployment-virtual-machines-and-containers/","summary":"Understanding VMs and containers, and how to manage them in Linux, is an important skill to master.","tags":"LFCS Linux Certificates","title":"LFCS - Operations Deployment - Virtual Machines and Containers"},{"content":"Running periodic tasks can be very helpful with system maintenance, or application development tasks. Linux uses cron jobs for this.\nSchedule Tasks to Run at Set Date and Time # cron # repetitive jobs # anacron (days, weeks, no smaller units) cat /etc/crontab # shows syntax and can set cronjob - systemwide table # cron # * matches all values # , match multiple 15,45 # - range 2-4 # / steps */4 which touch crontab -e # edits table of current user 35 6 * * * /user/bin/touch test crontab -l # list currents users crontabs sudo crontab -e -u jane # edits cron of another user (needs sudo) crontab -r # removes your crontab sudo crontab -r -u jane # removes for different user (root required) # special dirs /etc/cron.daily/ /etc/cron.hourly/ /etc/cron.monthly/ /etc/cron.weekly/ touch shellscript # no extension for cron sudo cp shellscript /etc/cron.daily/ # and make executable # after installing anacron sudo vim /etc/anacrontab anacron -T # verifies syntax sudo apt install at # single running jobs at \u0026#39;15:00\u0026#39; at \u0026#39;now + 3 hours\u0026#39; at \u0026#39;now + 3 days\u0026#39; at \u0026#39;now + 3 week\u0026#39; atq # query at -c 1 # show atrm 1 # removes A useful site to test your crontabs https://crontab.guru/\n","date":"11 July, 2025","id":2,"permalink":"/posts/lfcs-operations-deployment-task-scheduling/","summary":"Running periodic tasks can be very helpful with system maintenance, or application development tasks. Linux uses cron jobs for this.","tags":"LFCS Linux Certificates","title":"LFCS - Operations Deployment - Task Scheduling"},{"content":"Log files give you insight into your system and are very important for SRE and DevOps.\nHere are some of the most commonly used commands for managing and viewing system log files:\n# logging daemons # /var/log # rsyslog - rocket-fast system for log processing su --login grep -r \u0026#39;ssh\u0026#39; /var/log less /var/log/syslog # numbered #live view of log file tail -F /var/log/auth.log # follow mode # Systemd Journal daemon which sudo journalctl /user/bin/sudo # search logs for an app journalctl -u ssh.service # logs generate by ssh.service unit journalctl -e # end journalctl -f # follow # info warning err crit journalctl -p err journalctl -p info -g \u0026#39;^b\u0026#39; # grep journalctl -S 01:00 # since journalctl -S 01:00 -U 02:00 # since until journalctl -b 0 # boot zero journalctl -b -1 # 1 boot ago sudo mkdir /var/log/journal # will log historical boots last # last logins lastlog # each user last login ","date":"10 July, 2025","id":3,"permalink":"/posts/lfcs-operations-deployment-log-files/","summary":"Log files give you insight into your system and are very important for SRE and DevOps.","tags":"LFCS Linux Certificates","title":"LFCS - Operations Deployment - Log Files"},{"content":"Managing Linux processes is critical for DevOps and Linux administration!\nHere are some common commands to manage systemd services/unit files and system processes.\nStartup Process and Services # systemd is an init system # service units (tells init system about ) man systemd.service # look at unit file systemctl cat ssh.service sudo systemctl edit --full ssh.service # to edit the service unit sudo systemctl revert ssh.service # set back sudo systemctl status ssh.service sudo systemctl stop ssh.service sudo systemctl start ssh.service # start now sudo systemctl restart ssh.service # after editing config () sudo systemctl reload ssh.service # more graceful for users sudo systemctl reload-or-restart ssh.service # tries to reload sudo systemctl disabled ssh.service sudo systemctl is-enabled ssh.service sudo systemctl enable ssh.service # auto start sudo systemctl enable --now ssh.service # enable and start now sudo systemctl mask atd.service # prevent a service being started by another service sudo systemctl unmask atd.service sudo systemctl list-units --type service --all # all systemd units Systemd unit file # eg starting a custom app man systemd.service man systemd.unit # for unit part of file man systemd.exec man systemd.kill # look for Restart= option in man page and EXAMPLES ls /lib/systemd/system sudo cp /lib/systemd/system/ssh.service /etc/systemd/system/myservice.service vim myservice.service sudo systemctl daemon-reload sudo systemctl start myservice.service sudo journalctl -f # system log Diagnose and Manage Processes ps -a # unix syntax ps a # bsd syntax (not equivalent) ps # current terminal ps aux # ax - all, u - user (reminder \u0026#34;aux\u0026#34;illary) man ps # EXAMPLES # kernel processes wrapped in [] top # constantly reorders processes ps 1 # pid ps u 1 # user oriented format ps -U matt # for a specific user ps -U u matt # for a specific user pgrep -a syslog # process grep with name nice -n 11 bash # assigns priority ps lax # shows niceness ps fax # forest all (tree) ps faux # with user info nice -n -12 bash #permission denied (lower nice value) renice 7 1238 # pid id # only root can lower niceness # signals kill -L systemctl status ssh.service kill -SIGKILL 23434 # pid # all processes with name containing bash pgrep - a bash # check first pkill -KILL bash # kill all bash sleep 180 CTRL-Z # puts app in background fg # gets paused app back sleep 300 \u0026amp; # backgrounding a process jobs # checks background processes fg 1 # id bg # background again lsof -p 13536 # what files or dirs is process using sudo lsof /some/path # nothing is using file if not result ","date":"9 July, 2025","id":4,"permalink":"/posts/lfcs-operations-deployment-processes/","summary":"Managing Linux processes is critical for DevOps and Linux administration!","tags":"LFCS Linux Certificates","title":"LFCS - Operations Deployment - Processes"},{"content":"File redirection is a core part of Linux, this post covers file stdin, stdout, stderr and other commands.\nInput-Output Redirection # output redirection (file is overwritten) sort file.txt \u0026gt; sortedfile.txt sort file.txt 1\u0026gt; sortedfile.txt # equivalent stdout # appends sort file.txt \u0026gt;\u0026gt; sortedfile.txt #stdin \u0026lt; #stdout 1\u0026gt; #stderror 2\u0026gt; # redirect input \u0026lt; file.txt # redirect to stdout \u0026gt; file.txt or 1\u0026gt; file.txt # redirect error 2\u0026gt; error.txt # eg dont show errors with grep grep -r \u0026#39;^The\u0026#39; /etc/ 2\u0026gt;/dev/null # dev null is a black hole grep -r \u0026#39;^The\u0026#39; /etc/ 1\u0026gt;output.txt 2\u0026gt;/dev/null # can redirect to multiple places grep -r \u0026#39;^The\u0026#39; /etc/ 1\u0026gt;\u0026gt;output.txt 2\u0026gt;\u0026gt;/dev/null # append grep -r \u0026#39;^The\u0026#39; /etc/ 1\u0026gt;alloutput.txt 2\u0026gt;\u0026amp;1 # stderror goes into stdout (will all go to alloutput file) # input redirection # heredoc sort \u0026lt;\u0026lt;EOF \u0026gt;EOF # can be any word here # here string bc \u0026lt;\u0026lt;\u0026lt;1+2+3+4 10 # Piping grep -v \u0026#39;^#\u0026#39; /etc/ | sort | column -t ","date":"8 July, 2025","id":5,"permalink":"/posts/lfcs-essential-commands-file-redirection/","summary":"File redirection is a core part of Linux, this post covers file stdin, stdout, stderr and other commands.","tags":"LFCS Linux Certificates","title":"LFCS - Essential Commands - File redirecton"},{"content":"Continuing with the LFCS preparation series, this post covers various commands, including sed (stream editor), bundling and compression.\nCompare and Manipulate File Content cat /home/users.txt tac /home/users.txt # reversed tail -n 20 /home/users.txt # last 20 lines head -n 20 /home/users.txt # first 20 lines sed \u0026#39;s/canda/canada/g\u0026#39; userinfo.txt #stream editor - s is for substitute # file not edited yet -i # for --in-place cut -d \u0026#39; \u0026#39; -f 1 userinfo.txt # -d is delimiter to split columns by -f is fields (columns) uniq filename # removes unique lines next to each other sort filename | uniq # will get all diff file1 file2 diff -c file1 file2 # context diff -y file1 file2 # side by side #or sdiff file1 file2 Bundling / Packing # tar (tape archive) # tar is a packer and unpacker # ust packs/bundles doesnt compress # list contents of a tar file tar --list --file archive.tar tar -tf archive.tar tar tf archive.tar # create a new tar file with file1 tar --create --file archive.tar file1 tar cf archive.tar file1 # append a new tar file with file1 tar --append --file archive.tar file1 tar rf archive.tar file1 # append a new tar file with entire directory tar --create --file archive.tar Pictures/ # extract an tar (current dir) tar --extract --file archive.tar tar xf archive.tar # extract to another dir tar --extract --file archive.tar --directory /tmp/ tar xf archive.tar -C /tmp/ sudo tar xf archive.tar -C /tmp/ # makes sure permissions are restored Compression # these zips only work on a single file # all these commands create a zip file and delete original gzip file1 bzip2 file1 xz file1 # uncompress auto deletes zip and creates file gunzip file1.gz gzip --decompress file1.gz bunzip file1.gz bzip2 --decompress file1.bz2 unxz file1.gz xz --decompress file3.xz gzip --help # -k will keep original file # pack and compress entire dir zip -r archive.zip Pictures/ # -r is recursive unzip archive.zip # tar can compress with the tar --create --autocompress --file archive.tar file1 # there a specific options also ","date":"7 July, 2025","id":6,"permalink":"/posts/lfcs-essential-commands-file-content-bundling-and-compression/","summary":"Continuing with the LFCS preparation series, this post covers various commands, including sed (stream editor), bundling and compression.","tags":"LFCS Linux Certificates","title":"LFCS - Essential Commands - File content, bundling and compression"},{"content":"This post covers the commands for finding files on your system and searching the contents of files for patterns.\nSearch for files find [path] [params] # go-there find it (if you leave out path its current dir) find /usr/share/ -name \u0026#39;*.jpg\u0026#39; #regex of name find /lib64/ -size +10M # files greater than 10Meg find /dev/ -mmin -1 # modified in last minute find -iname felix # will find felix and Felix find -name \u0026#34;f*\u0026#34; # all files starting with f find -mmin [minute] #modified minute # can have multiple in search expression find -name \u0026#34;f*\u0026#34; -size 512k # implied AND find -name \u0026#34;f*\u0026#34; -o -size 512k # explicit OR find -not -name \u0026#34;f*\u0026#34; -o -size 512k # not being with f #or find \\! -name \u0026#34;f*\u0026#34; -o -size 512k # need to escape not find -perm 664 # file with these permissions find -perm -664 # file with at least these permissions find -perm /664 # any of these OR find -perm 664 #or find -perm u=rw,g=rw,o=r # exactly 664 permission find \\! -perm -o=r # files others cannot read Grep in files grep [options] \u0026#39;search_pattern\u0026#39; ./file # options are optional grep -i \u0026#39;password\u0026#39; ./config.txt # case insensitive grep -r \u0026#39;password\u0026#39; ./dir # recursive sudo grep -r --color \u0026#39;password\u0026#39; ./dir # sets colour grep -v \u0026#39;password\u0026#39; ./file # inverse search grep -w \u0026#39;password\u0026#39; ./file # word search (exact word) grep -o \u0026#39;password\u0026#39; ./file # only matching (just shows match not rest of line) Regex # operatiors ^ # line beginning with grep -v \u0026#39;^#\u0026#39; ./file # lines that dont begin with # -v is inverse $ # last character grep -w \u0026#39;7$\u0026#39; ./file # match word (single 7) . # any character \\ # escaping grep \u0026#39;\\.\u0026#39; ./file # escapes the period * # 0 - many times grep \u0026#39;let*\u0026#39; ./file # mactches # le # let # lett grep \u0026#39;/.*/\u0026#39; ./file # period any character * is any number of times, so this will match any string between / and / + # element exists at least once or more grep -r \u0026#39;0\\+\u0026#39; ./dir # need to escape the + (unless using extended regex) Extended Regex # You dont have to escape characters with Extended Regex grep -Er \u0026#39;0+\u0026#39; /etc/ # E with capital # egrep is same as grep -E egrep -r \u0026#39;0{3,}\u0026#39; /etc/ # min and max amount of repetitions egrep -r \u0026#39;10{,3}\u0026#39; /etc/ # string with 1 the at most 3 zeros egrep -r \u0026#39;0{3}\u0026#39; /etc/ # exactly 3 zeros egrep -r \u0026#39;disabled?\u0026#39; /etc/ # d is optional egrep -r \u0026#39;disabled|enabled\u0026#39; /etc/ # or egrep -r \u0026#39;c[au]t\u0026#39; /etc/ # range [a-z] set [az] egrep -r \u0026#39;/dev/[a-z]*\u0026#39; /etc/ # any number of letters from a-z egrep -r \u0026#39;/dev/[a-z]*[0-9]?\u0026#39; /etc/ # any number of letters from a-z, with optional 0-9 egrep -r \u0026#39;/dev/([a-z]*[0-9]?)*\u0026#39; /etc/ # subexpression (regex gets repeated 0 or more times) will match tty0p0 egrep -r \u0026#39;/dev/(([a-z]|[A-Z])*[0-9]?)*\u0026#39; /etc/ # subexpression (regex gets repeated 0 or more times) will match tty0p0 egrep -r \u0026#39;https[^:]\u0026#39; /etc/ # negated ranges egrep -r \u0026#39;http[^s:]\u0026#39; /etc/ # negated range set will only match http https://regexr.com/\n","date":"6 July, 2025","id":7,"permalink":"/posts/lfcs-essential-commands-file-searching/","summary":"This post covers the commands for finding files on your system and searching the contents of files for patterns.","tags":"LFCS Linux Certificates","title":"LFCS - Essential Commands - File searching"},{"content":"A summary of the commands learnt in the essentials section of the LFCS course preparation.\nFile management cd - # goes back to previous dir cd # goes to home dir cp -r [source] [dest] # copy dir with all its contents # Good idea to prepend directories with / cp -r testdir/ anotherdir/ # copies recursively to anotherdir #Hard links stat someimage.jpg # file info, will show link count (can be multiple) ln ./target_file ./link_file # hard link #Soft links #Hard links point to inodes, softlinks point to a path ln -s ./target_file ./link_file # soft link ls -l readlink ./link_file #soft link to dirs or different file sytem Owners, groups and permissions ls -l chgroup group_name fileordir # changes group groups # check the groups of user sudo chown jane file # change owner sudo chown aaron:family file # change user and group #Permissions chmod permissions file_or_dir # change mode # + is adding permissions # - is removing permissions # = exact permissions g=rw (declarative, will remove x if it exists) # u+ - user # g+ - group # o+ - other chmod u+w file # adding to user the write permission chmod u+rwx file # adding to user the rwx permissions chmod g+w file # adding to group the write permission chmod g+rwx file # adding to group the rwx permissions # split perm for user group and other chmod u+r,g=r,o= file # o has no permissions # or via decimal eg. 640 One decimal for each permission eg. 640 u | g | o 6 | 4 | 0 110 | 100 | 000 rw- | r-- | --- # SUID bit - Set User Identification bit (set on files) # when executed will be executed with userid of owner of file not by the person executing chmod 4664 suidfile # first 4 is suid permission ls -l suidfile -rwSrw-r-- # suid bit is S on the x permission (enabled but no executed permission) lower case s will be suid and x (applied with 4764) # SGID bit - Set Group Identification bit (set on files) chmod 2664 sgidfile # takes a 2 instead of 4 as first perm find . -perm /4000 # searching suid files find . -perm /2000 # searching sgid files chmod 6664 both # 4 (suid) \u0026amp; 2 (sguid) is 6 (for both) find . -perm /6000 ./sgidfile ./both ./suidfile #sticky bit for directories - for shared dirs - only people that can remove the file is owner and root chmod +t stickydir #or chmod 1777 stickydir # takes a 1 for sticky dir ls -ld # t or T on end of permissions chmod u+s,g+s,o+t file # sets sgid suid and sticky ","date":"4 July, 2025","id":8,"permalink":"/posts/lfcs-essential-commands-file-management-and-permissions/","summary":"A summary of the commands learnt in the essentials section of the LFCS course preparation.","tags":"LFCS Linux Certificates","title":"LFCS - Essential Commands - File management and permissions"},{"content":"The Linux Foundation Certified System Administrator (LFCS) exam is a 2 hour performance based exam in which you solve multiple issues from the command line.\nWhist I have been using Linux for almost a decade, I would like to formalise my knowledge and make sure all areas are covered. Learning Linux is an essential part of DevOps and SRE roles, so understanding its inner workings and how to manage a Linux system is essential.\nhttps://training.linuxfoundation.org/certification/linux-foundation-certified-sysadmin-lfcs/\nThere are other Linux certifications, namely the Linux Professional Institute LPIC-1 (101 \u0026amp; 102) certifications. These are multiple choice exams, and whilst this is still valuable, I feel the performance based tests will be a better reflection of knowledge and skill. I will likely still study the free LPIC content in future.\nhttps://www.lpi.org/our-certifications/lpic-1-overview/\nThere is also a really valuable YouTube series that will assist in preparation LPIC 1 - Video Series\nThe exam is broken up into the following sections:\nEssential Commands (20%) Operations Deployment (25%) Users and Groups (10%) Networking (25%) Storage(20%) Getting help directly on Linux Whilst information can be gleaned from the web, its an important skill to master being able to source answers locally (ie. on the system you are running on). Here are some of the most useful and common commands.\nMan Pages Man pages provide all the information you need. You can search specific sections depending on the documentation you require.\n# will list the sections man man # examples of getting information ls --help man ls man 1 printf man 3 printf Apropos command The apropos command searches all man pages related to a keyword or phrase. So if you are not sure where you should be searching this is a great tool.\n# searches man pages if you dont know what you need sudo mandb # create apropos db (man database) apropos tar # search for occurences of tar in manpages apropos -s 1,3 tar # sections Finding all executables (man section 1) with the term archiv\nhomelab on  master took 3s ⬢ [Docker] ❯ apropos -s 1 archiv apt-ftparchive (1) - Utility to generate index files ar (1) - create, modify, and extract from archives dpkg-deb (1) - Debian package archive (.deb) manipulation tool dpkg-split (1) - Debian package archive split/join tool funzip (1) - filter for extracting from a ZIP archive in a pipe git-archive (1) - Create an archive of files from a named tree git-bundle (1) - Move objects and refs by archive git-diagnose (1) - Generate a zip archive of diagnostic information git-get-tar-commit-id (1) - Extract commit ID from an archive created using git-archive git-index-pack (1) - Build pack index file for an existing packed archive git-pack-objects (1) - Create a packed archive of objects git-show-index (1) - Show packed archive index git-unpack-objects (1) - Unpack objects from a packed archive git-upload-archive (1) - Send archive back to git-archive git-verify-pack (1) - Validate packed Git archive files gp-archive (1) - archive gprofng experiment data gpg-zip (1) - encrypt or sign files into an archive gpgtar (1) - Encrypt or sign files into an archive ptardiff (1) - program that diffs an extracted archive against an unextracted one ptargrep (1) - Apply pattern matching to the contents of files in a tar archive ranlib (1) - generate an index to an archive tar (1) - an archiving utility tarcat (1) - concatenates the pieces of a GNU tar multi-volume archive unzip (1) - list, test and extract compressed files in a ZIP archive unzipsfx (1) - self-extracting stub for prepending to ZIP archives x86_64-linux-gnu-ar (1) - create, modify, and extract from archives x86_64-linux-gnu-gp-archive (1) - archive gprofng experiment data x86_64-linux-gnu-ranlib (1) - generate an index to an archive zip (1) - package and compress (archive) files zipgrep (1) - search files in a ZIP archive for lines matching a pattern zipinfo (1) - list detailed information about a ZIP archive Tldr-pages Man pages can sometimes be hard to digest, a simpler tool is tldr-pages, which is a collection of community-maintained help pages - which gives examples of how the tool is used.\nsudo apt install tldr tldr tar homelab on  master took 3s ⬢ [Docker] ❯ tldr tar Downloading tldr pages to /home/vscode/.local/share/tldr tar Archiving utility. Often combined with a compression method, such as gzip or bzip2. More information: https://www.gnu.org/software/tar. - [c]reate an archive and write it to a [f]ile: tar cf path/to/target.tar path/to/file1 path/to/file2 ... - [c]reate a g[z]ipped archive and write it to a [f]ile: tar czf path/to/target.tar.gz path/to/file1 path/to/file2 ... - [c]reate a g[z]ipped (compressed) archive from a directory using relative paths: tar czf path/to/target.tar.gz --directory=path/to/directory . - E[x]tract a (compressed) archive [f]ile into the current directory [v]erbosely: tar xvf path/to/source.tar[.gz|.bz2|.xz] - E[x]tract a (compressed) archive [f]ile into the target directory: tar xf path/to/source.tar[.gz|.bz2|.xz] --directory=path/to/directory - [c]reate a compressed archive and write it to a [f]ile, using the file extension to [a]utomatically determine the compression program: tar caf path/to/target.tar.xz path/to/file1 path/to/file2 ... - Lis[t] the contents of a tar [f]ile [v]erbosely: tar tvf path/to/source.tar https://github.com/tldr-pages/tldr\n","date":"1 July, 2025","id":9,"permalink":"/posts/preparation-for-the-lfcs-exam/","summary":"The Linux Foundation Certified System Administrator (LFCS) exam is a 2 hour performance based exam in which you solve multiple issues from the command line.","tags":"LFCS Linux Certificates","title":"Preparation for the LFCS exam"},{"content":"If we update our container registry with a new image, or a 3rd party app in our cluster needs updating\u0026hellip; that\u0026rsquo;s something to automate!\nWe will be using Renovate to automate the pulling in of new images. It will create a merge request in GitHub, and once we approve the MR, the image will be installed into our cluster!\nhttps://github.com/renovatebot/renovate\nOur project structure will look like this:\n├── apps │ ├── base │ ├── production │ └── staging ├── monitoring │ ├── config │ └── controllers │ ├── base │ └── kube-prometheus-stack │ └── staging │ └── kube-prometheus-stack ├── infrastructure │ └── controllers │ ├── base │ └── renovate │ └── staging │ └── renovate First make a new GitHub classic token with repo access, export it and store in a password manager.\nexport RENOVATE_TOKEN=ghp_123456789abcdefghijklmnopq kubectl create secret generic renovate-container-env \\ --from-literal=RENOVATE_TOKEN=ghp_123456789abcdefghijklmnopq \\ --dry-run=client \\ -o yaml \u0026gt; renovate-container-env.yaml Move renovate-container-env.yaml to base/renovate, and encrypt with sops.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place renovate-container-env.yaml Setup your other manifests:\n# infra/controllers/base/renovate/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: renovate # infra/controllers/base/renovate/cronjob.yaml apiVersion: batch/v1 kind: CronJob metadata: name: renovate namespace: renovate spec: schedule: \u0026#34;@hourly\u0026#34; concurrencyPolicy: Forbid jobTemplate: spec: template: spec: containers: - name: renovate image: renovate/renovate:latest args: - m4ttbr1tt/homelab envFrom: - secretRef: name: renovate-container-env - configMapRef: name: renovate-configmap restartPolicy: Never # infra/controllers/base/renovate/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: renovate-configmap namespace: renovate data: RENOVATE_AUTODISCOVER: \u0026#34;false\u0026#34; RENOVATE_GIT_AUTHOR: \u0026#34;Renovate Bot \u0026lt;bot@renovateapp.com\u0026gt;\u0026#34; RENOVATE_PLATFORM: \u0026#34;github\u0026#34; Add this renovate.json into your homelab git root:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://docs.renovatebot.com/renovate-schema.json\u0026#34;, \u0026#34;kubernetes\u0026#34;: { \u0026#34;fileMatch\u0026#34;: [ \u0026#34;\\\\.yaml$\u0026#34; ] } } Push and reconcile with flux.\nhttps://github.com/m4ttbr1tt/homelab\n","date":"30 June, 2025","id":10,"permalink":"/posts/automatic-image-updates-with-renovate/","summary":"If we update our container registry with a new image, or a 3rd party app in our cluster needs updating\u0026hellip; that\u0026rsquo;s something to automate!","tags":"homelab k3s GitOps","title":"Automatic image updates with Renovate"},{"content":"In the last post we port forwarded to the Grafana service, but that kinda sucks.\nI\u0026rsquo;m going to setup an ingress using the helm chart.\nFirst we determine the config for the helm chart by showing its values.\n# need to add repo to helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm show values prometheus-community/kube-prometheus-stack And then adding the necessary config to release.yaml values section.\ningress: enabled: true Force Flux reconcile\u0026hellip;\nflux reconcile kustomization monitoring-controllers --with-source Setup local dns\u0026hellip;\nsudo vim /etc/hosts 192.168.10.60 grafana.mattbritt.com We will now have internal network access to our dashboard!\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":11,"permalink":"/posts/ingress-with-traefik/","summary":"In the last post we port forwarded to the Grafana service, but that kinda sucks.","tags":"homelab k3s GitOps","title":"Ingress with Traefix"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n├── apps │ ├── base │ ├── production │ └── staging ├── monitoring │ ├── config │ └── controllers │ ├── base │ └── kube-prometheus-stack │ └── staging │ └── kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":12,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! 💥\n","date":"25 June, 2025","id":13,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":14,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! 💥\n","date":"18 June, 2025","id":15,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n├── apps │ ├── base │ ├── production │ └── staging ├── infrastructure │ ├── base │ ├── production │ └── staging └── clusters ├── production └── staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n├── apps │ ├── base │ ├── production │ └── staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time 😍\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":16,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful 😍.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":17,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! 😍\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":18,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while 😀\n","date":"7 June, 2025","id":19,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":20,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":21,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS… and I’m done. I’m moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut…\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI’m now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it’s time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":22,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS… and I’m done.  I’m moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":23,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":24,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I’m Matt Britt. I live in Cape Town, South Africa 🇿🇦😍. I’ve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I’ve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I’ve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I’ve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I’m learning, reflect on where I’ve been, and share thoughts on the technologies and practices that matter to me.\nI’ve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest 🤩\n","date":"12 May, 2025","id":25,"permalink":"/about/","summary":"Howzit, I’m Matt Britt. I live in Cape Town, South Africa 🇿🇦😍. I’ve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":26,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn’t mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they’re built into most editors via extensions.\nYou don’t need “full Vim”. Use VSCode with the Vim plugin. That’s more than enough to get the benefits.\nHere’s why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it’s essential\nThe learning curve can be steep. But it doesn’t have to be. Start with just a few motions. Use them until they’re second nature. Then add more.\nVim is a lifelong multiplier. There’s no excuse not to start.\n","date":"25 April, 2019","id":27,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn’t a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn’t know the layout, and more importantly, the keys didn’t match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":28,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn’t a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"Continuing with LFCS prep, we are moving on to user and group management\u0026hellip;\nUser management sudo adduser matt # will be prompted for password and info # adds user and group matt # home user is created # default shell is /bin/bash # copied from /etc/skel (like a template) sudo passwd matt sudo deluser matt sudo deluser --remove-home matt sudo adduser --shell /bin/othershell --home /home/otherdir/ matt cat /etc/passwd # for user id home and shell id ls -ln home # will print out user id id # shows the user and group id whoami # shows the current username sudo adduser --system --no-create-home sysacc # system account, intented for progams sudo usermod --home /home/otherdir --move-home matt # move and change home dir sudo usermod -d /home/otherdir -m matt # move and change home dir sudo usermod --login matt bob # change the users name or -l sudo usermod --shell /bin/othershell matt # or -s sudo usermod --lock matt # disables account or -L sudo usermod --unlock matt sudo usermod --expiredate 2028-12-10 matt # account expiry sudo chage --lastday 0 matt # change age of password (will force to login next login) sudo chage --maxdays 30 matt # force change every 30 days sudo chage --maxdays -1 matt # never expires sudo chage --list matt ![[Pasted image 20250708062741.png]]\nLocal Groups and Group Membership # each user can belong to one or more groups # used to manage permissions # user has primary / login group sudo groupadd developers sudo gpasswd --add matt developers # gpasswd is short for group password or -a groups matt # lists all groups that matt is part of sudo gpasswd --delete matt developors # or -d remove from group sudo usermod -g developers matt # sets the primary group sudo usermod --gid matt matt sudo groupmod --new-name programmers developers # rename group name or -n sudo groupdel programmers Manage System-Wide Environment Profiles printenv # or env # user specific env vars .bashrc sudo vim /etc/environment # system wide vars logout echo $SYSVAR # run something everytime any user logs in sudo vim /etc/profile.d/lastlogin.sh # create file Manage Template User Environment sudo vim /etc/skel/README # edit or create new files Configure User Resource Limits sudo /vim/etc/security/limits.conf # domain type item value # domain is user or group or * # type - hard, soft or - # item eg. nproc, fsize, cpu man limits.conf #limits.conf matt - nproc 3 # sudo -iu matt # real login ps | less # shell and these two are three process ls -a | grep bash | less # will not run as limited to 3 processes ulimit -a # see limits ulimit -u 5000 # lower limit or raise up to hard value Manage User Privileges groups # show groups for user # add user to sudo sudo gpasswd -a matt sudo # add user to sudo # edit sudoers file sudo visudo %sudo ALL=(ALL:ALL) ALL #group HOST=(RUNASUSER:RUNASGROUP) LISTOFCOMMANDS matt ALL=(ALL) ALL # no run as group sudo -u john ls /home/john # runs command as user john matt ALL=(john,jane) ALL # matt can only run commands as users john or jane matt ALL=(ALL) /bin/ls, /bin/stat # only certain commands matt ALL= /bin/ls, /bin/stat # only certain commands Manage Access to Root sudo ls /root # runs command as root sudo --login # login as root or -i logout su - # will ask for root passwd su -l # will ask for root passwd su --login # will ask for root passwd sudo passwd --lock root # sets password based logins Configure LDAP user and groups # user accounts are store in /etc/passwd # LDAP server (Lightweight Directory Access Protocol) # Single server change across multiple servers id john # no such user #lxc (linux containers) #lxd init (managers containers) lxc import ldap-server.tar.xz lxc list lxc start ldap-server sudo apt install libnss-ldap cat /etc/nsswitch.conf # name server switch, edited after config (where to find info) eg. files systemd ldap nslcd # name service local daemon (gets the actual data from the ldap server) sudo cat /etc/nslcd.conf # config file #nsl gets the info getent passwd --service ldap getent group --service ldap sudo pam-auth-update # to auto create user home (pluggable auth modules) ","date":"14 July, 2025","id":0,"permalink":"/posts/lfcs-users-and-groups/","summary":"Continuing with LFCS prep, we are moving on to user and group management\u0026hellip;","tags":"LFCS Linux Certificates","title":"LFCS - Users and Groups"},{"content":"Understanding VMs and containers, and how to manage them in Linux, is an important skill to master.\nHere is a list of commands that are useful for managing docker containers and QEMU-KVM VMs.\nCreate and Manage Containers docker images # may get a permission denied if not in correct user group docker search nginx docker pull nginx docker pull ubuntu/nginx docker images # list docker rmi ubuntu/nginx # remove images docker run nginx # starts a container, will be inside container docker run --detach --publish 8080:80 --name mycontainer nginx docker ps # lists running containers docker ps --all # all docker start 2342342343 # container id docker stop container_name nc localhost 8080 # connects to port 8080 with netcat docker rm container_name # removes container # order of cleanup docker stop mycontainer docker rm mycontainer docker rmi nginx # container to always restart docker run --detach --publish 8080:80 --name mywebserver nginx --restart always # create an image with Dockerfile FROM nginx COPY index.html /user/share/nginx/html/index.html docker build --tag mattb/customnginx:1.0 mydir Manage Virtual Machines # QEMU-KVM # Quick Emulator - Kernel-based Virtual Machine # virsh - manage virtual machines from the command line sudo apt install virt-manager # installs other cli utils # create an xml config (see online) virsh define testmachine.xml virsh help virsh list --all virsh start TestMachine virsh list virsh reboot TestMachine virsh reset TestMachine # force reset virsh shutdown TestMachine # graceful shutdown virsh destroy TestMachine # hard power off (does not destroy) virsh undefine TestMachine # will remove machine defined virsh autostart TestMachine # automstart when server boots virsh dominfo TestMachine # shows specs virsh set # tab tab # set cpus virsh help setvcpus virsh setvcpus TestMachine 2 --config virsh setvcpus TestMachine 2 --config --maximum # set memory virsh setmaxmem TestMachine 2048M --config virsh setmem TestMachine 2048M --config # download ubuntu image wget https://images......./release/ubuntu.img qemu-img info ubuntu.img qemu-img resize ubuntu.img 10G # changes the size of the virtual disk # add to default storage pools sudo cp ubuntu.img /var/lib/libvirt/images/ virt-install virt-install --osinfo list # lists types of os\u0026#39;s man virt-install # disk images has os preinstalled so we can just import not install! virt-install --help virt-install --osinfo ubuntu24.04 --name ubuntu1 --memory 3072 --vcpus 1 --import --disk /var/lib/libvirt/images/ubuntu.img --graphics none sudo apt install -y libguestfs-tools # setting root password virsh shutdown ubuntu1 sudo virt-customize -a /vir/lib/libvirt/images/ubuntu.img --root-password password:somepassword123 virsh start ubuntu1 virsh console ubuntu1 # reattach to console sudo apt install libosinfo-bin # check for latest os support osinfo-query os # what os are available for the tool # install a vm (without having a cloud image file) rather from an iso virt-install --osinfo debian12 --name debian1 --momory 1024 --vcpus 1 --disk size=10 --location /var/lib/libvirt/boot/debian.iso --graphics none --extra-args \u0026#34;console=ttyS0\u0026#34; # serial port # or you can use a url instead of iso virt-install --osinfo debian12 --name debian1 --momory 1024 --vcpus 1 --disk size=10 --location https://deb.debian.org/debian/dists/bookworm/main/installer-amd64/ --graphics none --extra-args \u0026#34;console=ttyS0\u0026#34; # serial port ","date":"12 July, 2025","id":1,"permalink":"/posts/lfcs-operations-deployment-virtual-machines-and-containers/","summary":"Understanding VMs and containers, and how to manage them in Linux, is an important skill to master.","tags":"LFCS Linux Certificates","title":"LFCS - Operations Deployment - Virtual Machines and Containers"},{"content":"Running periodic tasks can be very helpful with system maintenance, or application development tasks. Linux uses cron jobs for this.\nSchedule Tasks to Run at Set Date and Time # cron # repetitive jobs # anacron (days, weeks, no smaller units) cat /etc/crontab # shows syntax and can set cronjob - systemwide table # cron # * matches all values # , match multiple 15,45 # - range 2-4 # / steps */4 which touch crontab -e # edits table of current user 35 6 * * * /user/bin/touch test crontab -l # list currents users crontabs sudo crontab -e -u jane # edits cron of another user (needs sudo) crontab -r # removes your crontab sudo crontab -r -u jane # removes for different user (root required) # special dirs /etc/cron.daily/ /etc/cron.hourly/ /etc/cron.monthly/ /etc/cron.weekly/ touch shellscript # no extension for cron sudo cp shellscript /etc/cron.daily/ # and make executable # after installing anacron sudo vim /etc/anacrontab anacron -T # verifies syntax sudo apt install at # single running jobs at \u0026#39;15:00\u0026#39; at \u0026#39;now + 3 hours\u0026#39; at \u0026#39;now + 3 days\u0026#39; at \u0026#39;now + 3 week\u0026#39; atq # query at -c 1 # show atrm 1 # removes A useful site to test your crontabs https://crontab.guru/\n","date":"11 July, 2025","id":2,"permalink":"/posts/lfcs-operations-deployment-task-scheduling/","summary":"Running periodic tasks can be very helpful with system maintenance, or application development tasks. Linux uses cron jobs for this.","tags":"LFCS Linux Certificates","title":"LFCS - Operations Deployment - Task Scheduling"},{"content":"Log files give you insight into your system and are very important for SRE and DevOps.\nHere are some of the most commonly used commands for managing and viewing system log files:\n# logging daemons # /var/log # rsyslog - rocket-fast system for log processing su --login grep -r \u0026#39;ssh\u0026#39; /var/log less /var/log/syslog # numbered #live view of log file tail -F /var/log/auth.log # follow mode # Systemd Journal daemon which sudo journalctl /user/bin/sudo # search logs for an app journalctl -u ssh.service # logs generate by ssh.service unit journalctl -e # end journalctl -f # follow # info warning err crit journalctl -p err journalctl -p info -g \u0026#39;^b\u0026#39; # grep journalctl -S 01:00 # since journalctl -S 01:00 -U 02:00 # since until journalctl -b 0 # boot zero journalctl -b -1 # 1 boot ago sudo mkdir /var/log/journal # will log historical boots last # last logins lastlog # each user last login ","date":"10 July, 2025","id":3,"permalink":"/posts/lfcs-operations-deployment-log-files/","summary":"Log files give you insight into your system and are very important for SRE and DevOps.","tags":"LFCS Linux Certificates","title":"LFCS - Operations Deployment - Log Files"},{"content":"Managing Linux processes is critical for DevOps and Linux administration!\nHere are some common commands to manage systemd services/unit files and system processes.\nStartup Process and Services # systemd is an init system # service units (tells init system about ) man systemd.service # look at unit file systemctl cat ssh.service sudo systemctl edit --full ssh.service # to edit the service unit sudo systemctl revert ssh.service # set back sudo systemctl status ssh.service sudo systemctl stop ssh.service sudo systemctl start ssh.service # start now sudo systemctl restart ssh.service # after editing config () sudo systemctl reload ssh.service # more graceful for users sudo systemctl reload-or-restart ssh.service # tries to reload sudo systemctl disabled ssh.service sudo systemctl is-enabled ssh.service sudo systemctl enable ssh.service # auto start sudo systemctl enable --now ssh.service # enable and start now sudo systemctl mask atd.service # prevent a service being started by another service sudo systemctl unmask atd.service sudo systemctl list-units --type service --all # all systemd units Systemd unit file # eg starting a custom app man systemd.service man systemd.unit # for unit part of file man systemd.exec man systemd.kill # look for Restart= option in man page and EXAMPLES ls /lib/systemd/system sudo cp /lib/systemd/system/ssh.service /etc/systemd/system/myservice.service vim myservice.service sudo systemctl daemon-reload sudo systemctl start myservice.service sudo journalctl -f # system log Diagnose and Manage Processes ps -a # unix syntax ps a # bsd syntax (not equivalent) ps # current terminal ps aux # ax - all, u - user (reminder \u0026#34;aux\u0026#34;illary) man ps # EXAMPLES # kernel processes wrapped in [] top # constantly reorders processes ps 1 # pid ps u 1 # user oriented format ps -U matt # for a specific user ps -U u matt # for a specific user pgrep -a syslog # process grep with name nice -n 11 bash # assigns priority ps lax # shows niceness ps fax # forest all (tree) ps faux # with user info nice -n -12 bash #permission denied (lower nice value) renice 7 1238 # pid id # only root can lower niceness # signals kill -L systemctl status ssh.service kill -SIGKILL 23434 # pid # all processes with name containing bash pgrep - a bash # check first pkill -KILL bash # kill all bash sleep 180 CTRL-Z # puts app in background fg # gets paused app back sleep 300 \u0026amp; # backgrounding a process jobs # checks background processes fg 1 # id bg # background again lsof -p 13536 # what files or dirs is process using sudo lsof /some/path # nothing is using file if not result ","date":"9 July, 2025","id":4,"permalink":"/posts/lfcs-operations-deployment-processes/","summary":"Managing Linux processes is critical for DevOps and Linux administration!","tags":"LFCS Linux Certificates","title":"LFCS - Operations Deployment - Processes"},{"content":"File redirection is a core part of Linux, this post covers file stdin, stdout, stderr and other commands.\nInput-Output Redirection # output redirection (file is overwritten) sort file.txt \u0026gt; sortedfile.txt sort file.txt 1\u0026gt; sortedfile.txt # equivalent stdout # appends sort file.txt \u0026gt;\u0026gt; sortedfile.txt #stdin \u0026lt; #stdout 1\u0026gt; #stderror 2\u0026gt; # redirect input \u0026lt; file.txt # redirect to stdout \u0026gt; file.txt or 1\u0026gt; file.txt # redirect error 2\u0026gt; error.txt # eg dont show errors with grep grep -r \u0026#39;^The\u0026#39; /etc/ 2\u0026gt;/dev/null # dev null is a black hole grep -r \u0026#39;^The\u0026#39; /etc/ 1\u0026gt;output.txt 2\u0026gt;/dev/null # can redirect to multiple places grep -r \u0026#39;^The\u0026#39; /etc/ 1\u0026gt;\u0026gt;output.txt 2\u0026gt;\u0026gt;/dev/null # append grep -r \u0026#39;^The\u0026#39; /etc/ 1\u0026gt;alloutput.txt 2\u0026gt;\u0026amp;1 # stderror goes into stdout (will all go to alloutput file) # input redirection # heredoc sort \u0026lt;\u0026lt;EOF \u0026gt;EOF # can be any word here # here string bc \u0026lt;\u0026lt;\u0026lt;1+2+3+4 10 # Piping grep -v \u0026#39;^#\u0026#39; /etc/ | sort | column -t ","date":"8 July, 2025","id":5,"permalink":"/posts/lfcs-essential-commands-file-redirection/","summary":"File redirection is a core part of Linux, this post covers file stdin, stdout, stderr and other commands.","tags":"LFCS Linux Certificates","title":"LFCS - Essential Commands - File redirecton"},{"content":"Continuing with the LFCS preparation series, this post covers various commands, including sed (stream editor), bundling and compression.\nCompare and Manipulate File Content cat /home/users.txt tac /home/users.txt # reversed tail -n 20 /home/users.txt # last 20 lines head -n 20 /home/users.txt # first 20 lines sed \u0026#39;s/canda/canada/g\u0026#39; userinfo.txt #stream editor - s is for substitute # file not edited yet -i # for --in-place cut -d \u0026#39; \u0026#39; -f 1 userinfo.txt # -d is delimiter to split columns by -f is fields (columns) uniq filename # removes unique lines next to each other sort filename | uniq # will get all diff file1 file2 diff -c file1 file2 # context diff -y file1 file2 # side by side #or sdiff file1 file2 Bundling / Packing # tar (tape archive) # tar is a packer and unpacker # ust packs/bundles doesnt compress # list contents of a tar file tar --list --file archive.tar tar -tf archive.tar tar tf archive.tar # create a new tar file with file1 tar --create --file archive.tar file1 tar cf archive.tar file1 # append a new tar file with file1 tar --append --file archive.tar file1 tar rf archive.tar file1 # append a new tar file with entire directory tar --create --file archive.tar Pictures/ # extract an tar (current dir) tar --extract --file archive.tar tar xf archive.tar # extract to another dir tar --extract --file archive.tar --directory /tmp/ tar xf archive.tar -C /tmp/ sudo tar xf archive.tar -C /tmp/ # makes sure permissions are restored Compression # these zips only work on a single file # all these commands create a zip file and delete original gzip file1 bzip2 file1 xz file1 # uncompress auto deletes zip and creates file gunzip file1.gz gzip --decompress file1.gz bunzip file1.gz bzip2 --decompress file1.bz2 unxz file1.gz xz --decompress file3.xz gzip --help # -k will keep original file # pack and compress entire dir zip -r archive.zip Pictures/ # -r is recursive unzip archive.zip # tar can compress with the tar --create --autocompress --file archive.tar file1 # there a specific options also ","date":"7 July, 2025","id":6,"permalink":"/posts/lfcs-essential-commands-file-content-bundling-and-compression/","summary":"Continuing with the LFCS preparation series, this post covers various commands, including sed (stream editor), bundling and compression.","tags":"LFCS Linux Certificates","title":"LFCS - Essential Commands - File content, bundling and compression"},{"content":"This post covers the commands for finding files on your system and searching the contents of files for patterns.\nSearch for files find [path] [params] # go-there find it (if you leave out path its current dir) find /usr/share/ -name \u0026#39;*.jpg\u0026#39; #regex of name find /lib64/ -size +10M # files greater than 10Meg find /dev/ -mmin -1 # modified in last minute find -iname felix # will find felix and Felix find -name \u0026#34;f*\u0026#34; # all files starting with f find -mmin [minute] #modified minute # can have multiple in search expression find -name \u0026#34;f*\u0026#34; -size 512k # implied AND find -name \u0026#34;f*\u0026#34; -o -size 512k # explicit OR find -not -name \u0026#34;f*\u0026#34; -o -size 512k # not being with f #or find \\! -name \u0026#34;f*\u0026#34; -o -size 512k # need to escape not find -perm 664 # file with these permissions find -perm -664 # file with at least these permissions find -perm /664 # any of these OR find -perm 664 #or find -perm u=rw,g=rw,o=r # exactly 664 permission find \\! -perm -o=r # files others cannot read Grep in files grep [options] \u0026#39;search_pattern\u0026#39; ./file # options are optional grep -i \u0026#39;password\u0026#39; ./config.txt # case insensitive grep -r \u0026#39;password\u0026#39; ./dir # recursive sudo grep -r --color \u0026#39;password\u0026#39; ./dir # sets colour grep -v \u0026#39;password\u0026#39; ./file # inverse search grep -w \u0026#39;password\u0026#39; ./file # word search (exact word) grep -o \u0026#39;password\u0026#39; ./file # only matching (just shows match not rest of line) Regex # operatiors ^ # line beginning with grep -v \u0026#39;^#\u0026#39; ./file # lines that dont begin with # -v is inverse $ # last character grep -w \u0026#39;7$\u0026#39; ./file # match word (single 7) . # any character \\ # escaping grep \u0026#39;\\.\u0026#39; ./file # escapes the period * # 0 - many times grep \u0026#39;let*\u0026#39; ./file # mactches # le # let # lett grep \u0026#39;/.*/\u0026#39; ./file # period any character * is any number of times, so this will match any string between / and / + # element exists at least once or more grep -r \u0026#39;0\\+\u0026#39; ./dir # need to escape the + (unless using extended regex) Extended Regex # You dont have to escape characters with Extended Regex grep -Er \u0026#39;0+\u0026#39; /etc/ # E with capital # egrep is same as grep -E egrep -r \u0026#39;0{3,}\u0026#39; /etc/ # min and max amount of repetitions egrep -r \u0026#39;10{,3}\u0026#39; /etc/ # string with 1 the at most 3 zeros egrep -r \u0026#39;0{3}\u0026#39; /etc/ # exactly 3 zeros egrep -r \u0026#39;disabled?\u0026#39; /etc/ # d is optional egrep -r \u0026#39;disabled|enabled\u0026#39; /etc/ # or egrep -r \u0026#39;c[au]t\u0026#39; /etc/ # range [a-z] set [az] egrep -r \u0026#39;/dev/[a-z]*\u0026#39; /etc/ # any number of letters from a-z egrep -r \u0026#39;/dev/[a-z]*[0-9]?\u0026#39; /etc/ # any number of letters from a-z, with optional 0-9 egrep -r \u0026#39;/dev/([a-z]*[0-9]?)*\u0026#39; /etc/ # subexpression (regex gets repeated 0 or more times) will match tty0p0 egrep -r \u0026#39;/dev/(([a-z]|[A-Z])*[0-9]?)*\u0026#39; /etc/ # subexpression (regex gets repeated 0 or more times) will match tty0p0 egrep -r \u0026#39;https[^:]\u0026#39; /etc/ # negated ranges egrep -r \u0026#39;http[^s:]\u0026#39; /etc/ # negated range set will only match http https://regexr.com/\n","date":"6 July, 2025","id":7,"permalink":"/posts/lfcs-essential-commands-file-searching/","summary":"This post covers the commands for finding files on your system and searching the contents of files for patterns.","tags":"LFCS Linux Certificates","title":"LFCS - Essential Commands - File searching"},{"content":"A summary of the commands learnt in the essentials section of the LFCS course preparation.\nFile management cd - # goes back to previous dir cd # goes to home dir cp -r [source] [dest] # copy dir with all its contents # Good idea to prepend directories with / cp -r testdir/ anotherdir/ # copies recursively to anotherdir #Hard links stat someimage.jpg # file info, will show link count (can be multiple) ln ./target_file ./link_file # hard link #Soft links #Hard links point to inodes, softlinks point to a path ln -s ./target_file ./link_file # soft link ls -l readlink ./link_file #soft link to dirs or different file sytem Owners, groups and permissions ls -l chgroup group_name fileordir # changes group groups # check the groups of user sudo chown jane file # change owner sudo chown aaron:family file # change user and group #Permissions chmod permissions file_or_dir # change mode # + is adding permissions # - is removing permissions # = exact permissions g=rw (declarative, will remove x if it exists) # u+ - user # g+ - group # o+ - other chmod u+w file # adding to user the write permission chmod u+rwx file # adding to user the rwx permissions chmod g+w file # adding to group the write permission chmod g+rwx file # adding to group the rwx permissions # split perm for user group and other chmod u+r,g=r,o= file # o has no permissions # or via decimal eg. 640 One decimal for each permission eg. 640 u | g | o 6 | 4 | 0 110 | 100 | 000 rw- | r-- | --- # SUID bit - Set User Identification bit (set on files) # when executed will be executed with userid of owner of file not by the person executing chmod 4664 suidfile # first 4 is suid permission ls -l suidfile -rwSrw-r-- # suid bit is S on the x permission (enabled but no executed permission) lower case s will be suid and x (applied with 4764) # SGID bit - Set Group Identification bit (set on files) chmod 2664 sgidfile # takes a 2 instead of 4 as first perm find . -perm /4000 # searching suid files find . -perm /2000 # searching sgid files chmod 6664 both # 4 (suid) \u0026amp; 2 (sguid) is 6 (for both) find . -perm /6000 ./sgidfile ./both ./suidfile #sticky bit for directories - for shared dirs - only people that can remove the file is owner and root chmod +t stickydir #or chmod 1777 stickydir # takes a 1 for sticky dir ls -ld # t or T on end of permissions chmod u+s,g+s,o+t file # sets sgid suid and sticky ","date":"4 July, 2025","id":8,"permalink":"/posts/lfcs-essential-commands-file-management-and-permissions/","summary":"A summary of the commands learnt in the essentials section of the LFCS course preparation.","tags":"LFCS Linux Certificates","title":"LFCS - Essential Commands - File management and permissions"},{"content":"The Linux Foundation Certified System Administrator (LFCS) exam is a 2 hour performance based exam in which you solve multiple issues from the command line.\nWhist I have been using Linux for almost a decade, I would like to formalise my knowledge and make sure all areas are covered. Learning Linux is an essential part of DevOps and SRE roles, so understanding its inner workings and how to manage a Linux system is essential.\nhttps://training.linuxfoundation.org/certification/linux-foundation-certified-sysadmin-lfcs/\nThere are other Linux certifications, namely the Linux Professional Institute LPIC-1 (101 \u0026amp; 102) certifications. These are multiple choice exams, and whilst this is still valuable, I feel the performance based tests will be a better reflection of knowledge and skill. I will likely still study the free LPIC content in future.\nhttps://www.lpi.org/our-certifications/lpic-1-overview/\nThere is also a really valuable YouTube series that will assist in preparation LPIC 1 - Video Series\nThe exam is broken up into the following sections:\nEssential Commands (20%) Operations Deployment (25%) Users and Groups (10%) Networking (25%) Storage(20%) Getting help directly on Linux Whilst information can be gleaned from the web, its an important skill to master being able to source answers locally (ie. on the system you are running on). Here are some of the most useful and common commands.\nMan Pages Man pages provide all the information you need. You can search specific sections depending on the documentation you require.\n# will list the sections man man # examples of getting information ls --help man ls man 1 printf man 3 printf Apropos command The apropos command searches all man pages related to a keyword or phrase. So if you are not sure where you should be searching this is a great tool.\n# searches man pages if you dont know what you need sudo mandb # create apropos db (man database) apropos tar # search for occurences of tar in manpages apropos -s 1,3 tar # sections Finding all executables (man section 1) with the term archiv\nhomelab on  master took 3s ⬢ [Docker] ❯ apropos -s 1 archiv apt-ftparchive (1) - Utility to generate index files ar (1) - create, modify, and extract from archives dpkg-deb (1) - Debian package archive (.deb) manipulation tool dpkg-split (1) - Debian package archive split/join tool funzip (1) - filter for extracting from a ZIP archive in a pipe git-archive (1) - Create an archive of files from a named tree git-bundle (1) - Move objects and refs by archive git-diagnose (1) - Generate a zip archive of diagnostic information git-get-tar-commit-id (1) - Extract commit ID from an archive created using git-archive git-index-pack (1) - Build pack index file for an existing packed archive git-pack-objects (1) - Create a packed archive of objects git-show-index (1) - Show packed archive index git-unpack-objects (1) - Unpack objects from a packed archive git-upload-archive (1) - Send archive back to git-archive git-verify-pack (1) - Validate packed Git archive files gp-archive (1) - archive gprofng experiment data gpg-zip (1) - encrypt or sign files into an archive gpgtar (1) - Encrypt or sign files into an archive ptardiff (1) - program that diffs an extracted archive against an unextracted one ptargrep (1) - Apply pattern matching to the contents of files in a tar archive ranlib (1) - generate an index to an archive tar (1) - an archiving utility tarcat (1) - concatenates the pieces of a GNU tar multi-volume archive unzip (1) - list, test and extract compressed files in a ZIP archive unzipsfx (1) - self-extracting stub for prepending to ZIP archives x86_64-linux-gnu-ar (1) - create, modify, and extract from archives x86_64-linux-gnu-gp-archive (1) - archive gprofng experiment data x86_64-linux-gnu-ranlib (1) - generate an index to an archive zip (1) - package and compress (archive) files zipgrep (1) - search files in a ZIP archive for lines matching a pattern zipinfo (1) - list detailed information about a ZIP archive Tldr-pages Man pages can sometimes be hard to digest, a simpler tool is tldr-pages, which is a collection of community-maintained help pages - which gives examples of how the tool is used.\nsudo apt install tldr tldr tar homelab on  master took 3s ⬢ [Docker] ❯ tldr tar Downloading tldr pages to /home/vscode/.local/share/tldr tar Archiving utility. Often combined with a compression method, such as gzip or bzip2. More information: https://www.gnu.org/software/tar. - [c]reate an archive and write it to a [f]ile: tar cf path/to/target.tar path/to/file1 path/to/file2 ... - [c]reate a g[z]ipped archive and write it to a [f]ile: tar czf path/to/target.tar.gz path/to/file1 path/to/file2 ... - [c]reate a g[z]ipped (compressed) archive from a directory using relative paths: tar czf path/to/target.tar.gz --directory=path/to/directory . - E[x]tract a (compressed) archive [f]ile into the current directory [v]erbosely: tar xvf path/to/source.tar[.gz|.bz2|.xz] - E[x]tract a (compressed) archive [f]ile into the target directory: tar xf path/to/source.tar[.gz|.bz2|.xz] --directory=path/to/directory - [c]reate a compressed archive and write it to a [f]ile, using the file extension to [a]utomatically determine the compression program: tar caf path/to/target.tar.xz path/to/file1 path/to/file2 ... - Lis[t] the contents of a tar [f]ile [v]erbosely: tar tvf path/to/source.tar https://github.com/tldr-pages/tldr\n","date":"1 July, 2025","id":9,"permalink":"/posts/preparation-for-the-lfcs-exam/","summary":"The Linux Foundation Certified System Administrator (LFCS) exam is a 2 hour performance based exam in which you solve multiple issues from the command line.","tags":"LFCS Linux Certificates","title":"Preparation for the LFCS exam"},{"content":"If we update our container registry with a new image, or a 3rd party app in our cluster needs updating\u0026hellip; that\u0026rsquo;s something to automate!\nWe will be using Renovate to automate the pulling in of new images. It will create a merge request in GitHub, and once we approve the MR, the image will be installed into our cluster!\nhttps://github.com/renovatebot/renovate\nOur project structure will look like this:\n├── apps │ ├── base │ ├── production │ └── staging ├── monitoring │ ├── config │ └── controllers │ ├── base │ └── kube-prometheus-stack │ └── staging │ └── kube-prometheus-stack ├── infrastructure │ └── controllers │ ├── base │ └── renovate │ └── staging │ └── renovate First make a new GitHub classic token with repo access, export it and store in a password manager.\nexport RENOVATE_TOKEN=ghp_123456789abcdefghijklmnopq kubectl create secret generic renovate-container-env \\ --from-literal=RENOVATE_TOKEN=ghp_123456789abcdefghijklmnopq \\ --dry-run=client \\ -o yaml \u0026gt; renovate-container-env.yaml Move renovate-container-env.yaml to base/renovate, and encrypt with sops.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place renovate-container-env.yaml Setup your other manifests:\n# infra/controllers/base/renovate/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: renovate # infra/controllers/base/renovate/cronjob.yaml apiVersion: batch/v1 kind: CronJob metadata: name: renovate namespace: renovate spec: schedule: \u0026#34;@hourly\u0026#34; concurrencyPolicy: Forbid jobTemplate: spec: template: spec: containers: - name: renovate image: renovate/renovate:latest args: - m4ttbr1tt/homelab envFrom: - secretRef: name: renovate-container-env - configMapRef: name: renovate-configmap restartPolicy: Never # infra/controllers/base/renovate/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: renovate-configmap namespace: renovate data: RENOVATE_AUTODISCOVER: \u0026#34;false\u0026#34; RENOVATE_GIT_AUTHOR: \u0026#34;Renovate Bot \u0026lt;bot@renovateapp.com\u0026gt;\u0026#34; RENOVATE_PLATFORM: \u0026#34;github\u0026#34; Add this renovate.json into your homelab git root:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://docs.renovatebot.com/renovate-schema.json\u0026#34;, \u0026#34;kubernetes\u0026#34;: { \u0026#34;fileMatch\u0026#34;: [ \u0026#34;\\\\.yaml$\u0026#34; ] } } Push and reconcile with flux.\nhttps://github.com/m4ttbr1tt/homelab\n","date":"30 June, 2025","id":10,"permalink":"/posts/automatic-image-updates-with-renovate/","summary":"If we update our container registry with a new image, or a 3rd party app in our cluster needs updating\u0026hellip; that\u0026rsquo;s something to automate!","tags":"homelab k3s GitOps","title":"Automatic image updates with Renovate"},{"content":"In the last post we port forwarded to the Grafana service, but that kinda sucks.\nI\u0026rsquo;m going to setup an ingress using the helm chart.\nFirst we determine the config for the helm chart by showing its values.\n# need to add repo to helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm show values prometheus-community/kube-prometheus-stack And then adding the necessary config to release.yaml values section.\ningress: enabled: true Force Flux reconcile\u0026hellip;\nflux reconcile kustomization monitoring-controllers --with-source Setup local dns\u0026hellip;\nsudo vim /etc/hosts 192.168.10.60 grafana.mattbritt.com We will now have internal network access to our dashboard!\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":11,"permalink":"/posts/ingress-with-traefik/","summary":"In the last post we port forwarded to the Grafana service, but that kinda sucks.","tags":"homelab k3s GitOps","title":"Ingress with Traefix"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n├── apps │ ├── base │ ├── production │ └── staging ├── monitoring │ ├── config │ └── controllers │ ├── base │ └── kube-prometheus-stack │ └── staging │ └── kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":12,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! 💥\n","date":"25 June, 2025","id":13,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":14,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! 💥\n","date":"18 June, 2025","id":15,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n├── apps │ ├── base │ ├── production │ └── staging ├── infrastructure │ ├── base │ ├── production │ └── staging └── clusters ├── production └── staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n├── apps │ ├── base │ ├── production │ └── staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time 😍\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":16,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful 😍.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":17,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! 😍\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":18,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while 😀\n","date":"7 June, 2025","id":19,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":20,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":21,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS… and I’m done. I’m moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut…\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI’m now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it’s time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":22,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS… and I’m done.  I’m moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":23,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":24,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I’m Matt Britt. I live in Cape Town, South Africa 🇿🇦😍. I’ve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I’ve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I’ve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I’ve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I’m learning, reflect on where I’ve been, and share thoughts on the technologies and practices that matter to me.\nI’ve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest 🤩\n","date":"12 May, 2025","id":25,"permalink":"/about/","summary":"Howzit, I’m Matt Britt. I live in Cape Town, South Africa 🇿🇦😍. I’ve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":26,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn’t mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they’re built into most editors via extensions.\nYou don’t need “full Vim”. Use VSCode with the Vim plugin. That’s more than enough to get the benefits.\nHere’s why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it’s essential\nThe learning curve can be steep. But it doesn’t have to be. Start with just a few motions. Use them until they’re second nature. Then add more.\nVim is a lifelong multiplier. There’s no excuse not to start.\n","date":"25 April, 2019","id":27,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn’t a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn’t know the layout, and more importantly, the keys didn’t match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":28,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn’t a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"Continuing with LFCS prep, we are moving on to user and group management\u0026hellip;\nUser management sudo adduser matt # will be prompted for password and info # adds user and group matt # home user is created # default shell is /bin/bash # copied from /etc/skel (like a template) sudo passwd matt sudo deluser matt sudo deluser --remove-home matt sudo adduser --shell /bin/othershell --home /home/otherdir/ matt cat /etc/passwd # for user id home and shell id ls -ln home # will print out user id id # shows the user and group id whoami # shows the current username sudo adduser --system --no-create-home sysacc # system account, intented for progams sudo usermod --home /home/otherdir --move-home matt # move and change home dir sudo usermod -d /home/otherdir -m matt # move and change home dir sudo usermod --login matt bob # change the users name or -l sudo usermod --shell /bin/othershell matt # or -s sudo usermod --lock matt # disables account or -L sudo usermod --unlock matt sudo usermod --expiredate 2028-12-10 matt # account expiry sudo chage --lastday 0 matt # change age of password (will force to login next login) sudo chage --maxdays 30 matt # force change every 30 days sudo chage --maxdays -1 matt # never expires sudo chage --list matt Local Groups and Group Membership # each user can belong to one or more groups # used to manage permissions # user has primary / login group sudo groupadd developers sudo gpasswd --add matt developers # gpasswd is short for group password or -a groups matt # lists all groups that matt is part of sudo gpasswd --delete matt developors # or -d remove from group sudo usermod -g developers matt # sets the primary group sudo usermod --gid matt matt sudo groupmod --new-name programmers developers # rename group name or -n sudo groupdel programmers Manage System-Wide Environment Profiles printenv # or env # user specific env vars .bashrc sudo vim /etc/environment # system wide vars logout echo $SYSVAR # run something everytime any user logs in sudo vim /etc/profile.d/lastlogin.sh # create file Manage Template User Environment sudo vim /etc/skel/README # edit or create new files Configure User Resource Limits sudo /vim/etc/security/limits.conf # domain type item value # domain is user or group or * # type - hard, soft or - # item eg. nproc, fsize, cpu man limits.conf #limits.conf matt - nproc 3 # sudo -iu matt # real login ps | less # shell and these two are three process ls -a | grep bash | less # will not run as limited to 3 processes ulimit -a # see limits ulimit -u 5000 # lower limit or raise up to hard value Manage User Privileges groups # show groups for user # add user to sudo sudo gpasswd -a matt sudo # add user to sudo # edit sudoers file sudo visudo %sudo ALL=(ALL:ALL) ALL #group HOST=(RUNASUSER:RUNASGROUP) LISTOFCOMMANDS matt ALL=(ALL) ALL # no run as group sudo -u john ls /home/john # runs command as user john matt ALL=(john,jane) ALL # matt can only run commands as users john or jane matt ALL=(ALL) /bin/ls, /bin/stat # only certain commands matt ALL= /bin/ls, /bin/stat # only certain commands Manage Access to Root sudo ls /root # runs command as root sudo --login # login as root or -i logout su - # will ask for root passwd su -l # will ask for root passwd su --login # will ask for root passwd sudo passwd --lock root # sets password based logins Configure LDAP user and groups # user accounts are store in /etc/passwd # LDAP server (Lightweight Directory Access Protocol) # Single server change across multiple servers id john # no such user #lxc (linux containers) #lxd init (managers containers) lxc import ldap-server.tar.xz lxc list lxc start ldap-server sudo apt install libnss-ldap cat /etc/nsswitch.conf # name server switch, edited after config (where to find info) eg. files systemd ldap nslcd # name service local daemon (gets the actual data from the ldap server) sudo cat /etc/nslcd.conf # config file #nsl gets the info getent passwd --service ldap getent group --service ldap sudo pam-auth-update # to auto create user home (pluggable auth modules) ","date":"14 July, 2025","id":0,"permalink":"/posts/lfcs-users-and-groups/","summary":"Continuing with LFCS prep, we are moving on to user and group management\u0026hellip;","tags":"LFCS Linux Certificates","title":"LFCS - Users and Groups"},{"content":"Understanding VMs and containers, and how to manage them in Linux, is an important skill to master.\nHere is a list of commands that are useful for managing docker containers and QEMU-KVM VMs.\nCreate and Manage Containers docker images # may get a permission denied if not in correct user group docker search nginx docker pull nginx docker pull ubuntu/nginx docker images # list docker rmi ubuntu/nginx # remove images docker run nginx # starts a container, will be inside container docker run --detach --publish 8080:80 --name mycontainer nginx docker ps # lists running containers docker ps --all # all docker start 2342342343 # container id docker stop container_name nc localhost 8080 # connects to port 8080 with netcat docker rm container_name # removes container # order of cleanup docker stop mycontainer docker rm mycontainer docker rmi nginx # container to always restart docker run --detach --publish 8080:80 --name mywebserver nginx --restart always # create an image with Dockerfile FROM nginx COPY index.html /user/share/nginx/html/index.html docker build --tag mattb/customnginx:1.0 mydir Manage Virtual Machines # QEMU-KVM # Quick Emulator - Kernel-based Virtual Machine # virsh - manage virtual machines from the command line sudo apt install virt-manager # installs other cli utils # create an xml config (see online) virsh define testmachine.xml virsh help virsh list --all virsh start TestMachine virsh list virsh reboot TestMachine virsh reset TestMachine # force reset virsh shutdown TestMachine # graceful shutdown virsh destroy TestMachine # hard power off (does not destroy) virsh undefine TestMachine # will remove machine defined virsh autostart TestMachine # automstart when server boots virsh dominfo TestMachine # shows specs virsh set # tab tab # set cpus virsh help setvcpus virsh setvcpus TestMachine 2 --config virsh setvcpus TestMachine 2 --config --maximum # set memory virsh setmaxmem TestMachine 2048M --config virsh setmem TestMachine 2048M --config # download ubuntu image wget https://images......./release/ubuntu.img qemu-img info ubuntu.img qemu-img resize ubuntu.img 10G # changes the size of the virtual disk # add to default storage pools sudo cp ubuntu.img /var/lib/libvirt/images/ virt-install virt-install --osinfo list # lists types of os\u0026#39;s man virt-install # disk images has os preinstalled so we can just import not install! virt-install --help virt-install --osinfo ubuntu24.04 --name ubuntu1 --memory 3072 --vcpus 1 --import --disk /var/lib/libvirt/images/ubuntu.img --graphics none sudo apt install -y libguestfs-tools # setting root password virsh shutdown ubuntu1 sudo virt-customize -a /vir/lib/libvirt/images/ubuntu.img --root-password password:somepassword123 virsh start ubuntu1 virsh console ubuntu1 # reattach to console sudo apt install libosinfo-bin # check for latest os support osinfo-query os # what os are available for the tool # install a vm (without having a cloud image file) rather from an iso virt-install --osinfo debian12 --name debian1 --momory 1024 --vcpus 1 --disk size=10 --location /var/lib/libvirt/boot/debian.iso --graphics none --extra-args \u0026#34;console=ttyS0\u0026#34; # serial port # or you can use a url instead of iso virt-install --osinfo debian12 --name debian1 --momory 1024 --vcpus 1 --disk size=10 --location https://deb.debian.org/debian/dists/bookworm/main/installer-amd64/ --graphics none --extra-args \u0026#34;console=ttyS0\u0026#34; # serial port ","date":"12 July, 2025","id":1,"permalink":"/posts/lfcs-operations-deployment-virtual-machines-and-containers/","summary":"Understanding VMs and containers, and how to manage them in Linux, is an important skill to master.","tags":"LFCS Linux Certificates","title":"LFCS - Operations Deployment - Virtual Machines and Containers"},{"content":"Running periodic tasks can be very helpful with system maintenance, or application development tasks. Linux uses cron jobs for this.\nSchedule Tasks to Run at Set Date and Time # cron # repetitive jobs # anacron (days, weeks, no smaller units) cat /etc/crontab # shows syntax and can set cronjob - systemwide table # cron # * matches all values # , match multiple 15,45 # - range 2-4 # / steps */4 which touch crontab -e # edits table of current user 35 6 * * * /user/bin/touch test crontab -l # list currents users crontabs sudo crontab -e -u jane # edits cron of another user (needs sudo) crontab -r # removes your crontab sudo crontab -r -u jane # removes for different user (root required) # special dirs /etc/cron.daily/ /etc/cron.hourly/ /etc/cron.monthly/ /etc/cron.weekly/ touch shellscript # no extension for cron sudo cp shellscript /etc/cron.daily/ # and make executable # after installing anacron sudo vim /etc/anacrontab anacron -T # verifies syntax sudo apt install at # single running jobs at \u0026#39;15:00\u0026#39; at \u0026#39;now + 3 hours\u0026#39; at \u0026#39;now + 3 days\u0026#39; at \u0026#39;now + 3 week\u0026#39; atq # query at -c 1 # show atrm 1 # removes A useful site to test your crontabs https://crontab.guru/\n","date":"11 July, 2025","id":2,"permalink":"/posts/lfcs-operations-deployment-task-scheduling/","summary":"Running periodic tasks can be very helpful with system maintenance, or application development tasks. Linux uses cron jobs for this.","tags":"LFCS Linux Certificates","title":"LFCS - Operations Deployment - Task Scheduling"},{"content":"Log files give you insight into your system and are very important for SRE and DevOps.\nHere are some of the most commonly used commands for managing and viewing system log files:\n# logging daemons # /var/log # rsyslog - rocket-fast system for log processing su --login grep -r \u0026#39;ssh\u0026#39; /var/log less /var/log/syslog # numbered #live view of log file tail -F /var/log/auth.log # follow mode # Systemd Journal daemon which sudo journalctl /user/bin/sudo # search logs for an app journalctl -u ssh.service # logs generate by ssh.service unit journalctl -e # end journalctl -f # follow # info warning err crit journalctl -p err journalctl -p info -g \u0026#39;^b\u0026#39; # grep journalctl -S 01:00 # since journalctl -S 01:00 -U 02:00 # since until journalctl -b 0 # boot zero journalctl -b -1 # 1 boot ago sudo mkdir /var/log/journal # will log historical boots last # last logins lastlog # each user last login ","date":"10 July, 2025","id":3,"permalink":"/posts/lfcs-operations-deployment-log-files/","summary":"Log files give you insight into your system and are very important for SRE and DevOps.","tags":"LFCS Linux Certificates","title":"LFCS - Operations Deployment - Log Files"},{"content":"Managing Linux processes is critical for DevOps and Linux administration!\nHere are some common commands to manage systemd services/unit files and system processes.\nStartup Process and Services # systemd is an init system # service units (tells init system about ) man systemd.service # look at unit file systemctl cat ssh.service sudo systemctl edit --full ssh.service # to edit the service unit sudo systemctl revert ssh.service # set back sudo systemctl status ssh.service sudo systemctl stop ssh.service sudo systemctl start ssh.service # start now sudo systemctl restart ssh.service # after editing config () sudo systemctl reload ssh.service # more graceful for users sudo systemctl reload-or-restart ssh.service # tries to reload sudo systemctl disabled ssh.service sudo systemctl is-enabled ssh.service sudo systemctl enable ssh.service # auto start sudo systemctl enable --now ssh.service # enable and start now sudo systemctl mask atd.service # prevent a service being started by another service sudo systemctl unmask atd.service sudo systemctl list-units --type service --all # all systemd units Systemd unit file # eg starting a custom app man systemd.service man systemd.unit # for unit part of file man systemd.exec man systemd.kill # look for Restart= option in man page and EXAMPLES ls /lib/systemd/system sudo cp /lib/systemd/system/ssh.service /etc/systemd/system/myservice.service vim myservice.service sudo systemctl daemon-reload sudo systemctl start myservice.service sudo journalctl -f # system log Diagnose and Manage Processes ps -a # unix syntax ps a # bsd syntax (not equivalent) ps # current terminal ps aux # ax - all, u - user (reminder \u0026#34;aux\u0026#34;illary) man ps # EXAMPLES # kernel processes wrapped in [] top # constantly reorders processes ps 1 # pid ps u 1 # user oriented format ps -U matt # for a specific user ps -U u matt # for a specific user pgrep -a syslog # process grep with name nice -n 11 bash # assigns priority ps lax # shows niceness ps fax # forest all (tree) ps faux # with user info nice -n -12 bash #permission denied (lower nice value) renice 7 1238 # pid id # only root can lower niceness # signals kill -L systemctl status ssh.service kill -SIGKILL 23434 # pid # all processes with name containing bash pgrep - a bash # check first pkill -KILL bash # kill all bash sleep 180 CTRL-Z # puts app in background fg # gets paused app back sleep 300 \u0026amp; # backgrounding a process jobs # checks background processes fg 1 # id bg # background again lsof -p 13536 # what files or dirs is process using sudo lsof /some/path # nothing is using file if not result ","date":"9 July, 2025","id":4,"permalink":"/posts/lfcs-operations-deployment-processes/","summary":"Managing Linux processes is critical for DevOps and Linux administration!","tags":"LFCS Linux Certificates","title":"LFCS - Operations Deployment - Processes"},{"content":"File redirection is a core part of Linux, this post covers file stdin, stdout, stderr and other commands.\nInput-Output Redirection # output redirection (file is overwritten) sort file.txt \u0026gt; sortedfile.txt sort file.txt 1\u0026gt; sortedfile.txt # equivalent stdout # appends sort file.txt \u0026gt;\u0026gt; sortedfile.txt #stdin \u0026lt; #stdout 1\u0026gt; #stderror 2\u0026gt; # redirect input \u0026lt; file.txt # redirect to stdout \u0026gt; file.txt or 1\u0026gt; file.txt # redirect error 2\u0026gt; error.txt # eg dont show errors with grep grep -r \u0026#39;^The\u0026#39; /etc/ 2\u0026gt;/dev/null # dev null is a black hole grep -r \u0026#39;^The\u0026#39; /etc/ 1\u0026gt;output.txt 2\u0026gt;/dev/null # can redirect to multiple places grep -r \u0026#39;^The\u0026#39; /etc/ 1\u0026gt;\u0026gt;output.txt 2\u0026gt;\u0026gt;/dev/null # append grep -r \u0026#39;^The\u0026#39; /etc/ 1\u0026gt;alloutput.txt 2\u0026gt;\u0026amp;1 # stderror goes into stdout (will all go to alloutput file) # input redirection # heredoc sort \u0026lt;\u0026lt;EOF \u0026gt;EOF # can be any word here # here string bc \u0026lt;\u0026lt;\u0026lt;1+2+3+4 10 # Piping grep -v \u0026#39;^#\u0026#39; /etc/ | sort | column -t ","date":"8 July, 2025","id":5,"permalink":"/posts/lfcs-essential-commands-file-redirection/","summary":"File redirection is a core part of Linux, this post covers file stdin, stdout, stderr and other commands.","tags":"LFCS Linux Certificates","title":"LFCS - Essential Commands - File redirecton"},{"content":"Continuing with the LFCS preparation series, this post covers various commands, including sed (stream editor), bundling and compression.\nCompare and Manipulate File Content cat /home/users.txt tac /home/users.txt # reversed tail -n 20 /home/users.txt # last 20 lines head -n 20 /home/users.txt # first 20 lines sed \u0026#39;s/canda/canada/g\u0026#39; userinfo.txt #stream editor - s is for substitute # file not edited yet -i # for --in-place cut -d \u0026#39; \u0026#39; -f 1 userinfo.txt # -d is delimiter to split columns by -f is fields (columns) uniq filename # removes unique lines next to each other sort filename | uniq # will get all diff file1 file2 diff -c file1 file2 # context diff -y file1 file2 # side by side #or sdiff file1 file2 Bundling / Packing # tar (tape archive) # tar is a packer and unpacker # ust packs/bundles doesnt compress # list contents of a tar file tar --list --file archive.tar tar -tf archive.tar tar tf archive.tar # create a new tar file with file1 tar --create --file archive.tar file1 tar cf archive.tar file1 # append a new tar file with file1 tar --append --file archive.tar file1 tar rf archive.tar file1 # append a new tar file with entire directory tar --create --file archive.tar Pictures/ # extract an tar (current dir) tar --extract --file archive.tar tar xf archive.tar # extract to another dir tar --extract --file archive.tar --directory /tmp/ tar xf archive.tar -C /tmp/ sudo tar xf archive.tar -C /tmp/ # makes sure permissions are restored Compression # these zips only work on a single file # all these commands create a zip file and delete original gzip file1 bzip2 file1 xz file1 # uncompress auto deletes zip and creates file gunzip file1.gz gzip --decompress file1.gz bunzip file1.gz bzip2 --decompress file1.bz2 unxz file1.gz xz --decompress file3.xz gzip --help # -k will keep original file # pack and compress entire dir zip -r archive.zip Pictures/ # -r is recursive unzip archive.zip # tar can compress with the tar --create --autocompress --file archive.tar file1 # there a specific options also ","date":"7 July, 2025","id":6,"permalink":"/posts/lfcs-essential-commands-file-content-bundling-and-compression/","summary":"Continuing with the LFCS preparation series, this post covers various commands, including sed (stream editor), bundling and compression.","tags":"LFCS Linux Certificates","title":"LFCS - Essential Commands - File content, bundling and compression"},{"content":"This post covers the commands for finding files on your system and searching the contents of files for patterns.\nSearch for files find [path] [params] # go-there find it (if you leave out path its current dir) find /usr/share/ -name \u0026#39;*.jpg\u0026#39; #regex of name find /lib64/ -size +10M # files greater than 10Meg find /dev/ -mmin -1 # modified in last minute find -iname felix # will find felix and Felix find -name \u0026#34;f*\u0026#34; # all files starting with f find -mmin [minute] #modified minute # can have multiple in search expression find -name \u0026#34;f*\u0026#34; -size 512k # implied AND find -name \u0026#34;f*\u0026#34; -o -size 512k # explicit OR find -not -name \u0026#34;f*\u0026#34; -o -size 512k # not being with f #or find \\! -name \u0026#34;f*\u0026#34; -o -size 512k # need to escape not find -perm 664 # file with these permissions find -perm -664 # file with at least these permissions find -perm /664 # any of these OR find -perm 664 #or find -perm u=rw,g=rw,o=r # exactly 664 permission find \\! -perm -o=r # files others cannot read Grep in files grep [options] \u0026#39;search_pattern\u0026#39; ./file # options are optional grep -i \u0026#39;password\u0026#39; ./config.txt # case insensitive grep -r \u0026#39;password\u0026#39; ./dir # recursive sudo grep -r --color \u0026#39;password\u0026#39; ./dir # sets colour grep -v \u0026#39;password\u0026#39; ./file # inverse search grep -w \u0026#39;password\u0026#39; ./file # word search (exact word) grep -o \u0026#39;password\u0026#39; ./file # only matching (just shows match not rest of line) Regex # operatiors ^ # line beginning with grep -v \u0026#39;^#\u0026#39; ./file # lines that dont begin with # -v is inverse $ # last character grep -w \u0026#39;7$\u0026#39; ./file # match word (single 7) . # any character \\ # escaping grep \u0026#39;\\.\u0026#39; ./file # escapes the period * # 0 - many times grep \u0026#39;let*\u0026#39; ./file # mactches # le # let # lett grep \u0026#39;/.*/\u0026#39; ./file # period any character * is any number of times, so this will match any string between / and / + # element exists at least once or more grep -r \u0026#39;0\\+\u0026#39; ./dir # need to escape the + (unless using extended regex) Extended Regex # You dont have to escape characters with Extended Regex grep -Er \u0026#39;0+\u0026#39; /etc/ # E with capital # egrep is same as grep -E egrep -r \u0026#39;0{3,}\u0026#39; /etc/ # min and max amount of repetitions egrep -r \u0026#39;10{,3}\u0026#39; /etc/ # string with 1 the at most 3 zeros egrep -r \u0026#39;0{3}\u0026#39; /etc/ # exactly 3 zeros egrep -r \u0026#39;disabled?\u0026#39; /etc/ # d is optional egrep -r \u0026#39;disabled|enabled\u0026#39; /etc/ # or egrep -r \u0026#39;c[au]t\u0026#39; /etc/ # range [a-z] set [az] egrep -r \u0026#39;/dev/[a-z]*\u0026#39; /etc/ # any number of letters from a-z egrep -r \u0026#39;/dev/[a-z]*[0-9]?\u0026#39; /etc/ # any number of letters from a-z, with optional 0-9 egrep -r \u0026#39;/dev/([a-z]*[0-9]?)*\u0026#39; /etc/ # subexpression (regex gets repeated 0 or more times) will match tty0p0 egrep -r \u0026#39;/dev/(([a-z]|[A-Z])*[0-9]?)*\u0026#39; /etc/ # subexpression (regex gets repeated 0 or more times) will match tty0p0 egrep -r \u0026#39;https[^:]\u0026#39; /etc/ # negated ranges egrep -r \u0026#39;http[^s:]\u0026#39; /etc/ # negated range set will only match http https://regexr.com/\n","date":"6 July, 2025","id":7,"permalink":"/posts/lfcs-essential-commands-file-searching/","summary":"This post covers the commands for finding files on your system and searching the contents of files for patterns.","tags":"LFCS Linux Certificates","title":"LFCS - Essential Commands - File searching"},{"content":"A summary of the commands learnt in the essentials section of the LFCS course preparation.\nFile management cd - # goes back to previous dir cd # goes to home dir cp -r [source] [dest] # copy dir with all its contents # Good idea to prepend directories with / cp -r testdir/ anotherdir/ # copies recursively to anotherdir #Hard links stat someimage.jpg # file info, will show link count (can be multiple) ln ./target_file ./link_file # hard link #Soft links #Hard links point to inodes, softlinks point to a path ln -s ./target_file ./link_file # soft link ls -l readlink ./link_file #soft link to dirs or different file sytem Owners, groups and permissions ls -l chgroup group_name fileordir # changes group groups # check the groups of user sudo chown jane file # change owner sudo chown aaron:family file # change user and group #Permissions chmod permissions file_or_dir # change mode # + is adding permissions # - is removing permissions # = exact permissions g=rw (declarative, will remove x if it exists) # u+ - user # g+ - group # o+ - other chmod u+w file # adding to user the write permission chmod u+rwx file # adding to user the rwx permissions chmod g+w file # adding to group the write permission chmod g+rwx file # adding to group the rwx permissions # split perm for user group and other chmod u+r,g=r,o= file # o has no permissions # or via decimal eg. 640 One decimal for each permission eg. 640 u | g | o 6 | 4 | 0 110 | 100 | 000 rw- | r-- | --- # SUID bit - Set User Identification bit (set on files) # when executed will be executed with userid of owner of file not by the person executing chmod 4664 suidfile # first 4 is suid permission ls -l suidfile -rwSrw-r-- # suid bit is S on the x permission (enabled but no executed permission) lower case s will be suid and x (applied with 4764) # SGID bit - Set Group Identification bit (set on files) chmod 2664 sgidfile # takes a 2 instead of 4 as first perm find . -perm /4000 # searching suid files find . -perm /2000 # searching sgid files chmod 6664 both # 4 (suid) \u0026amp; 2 (sguid) is 6 (for both) find . -perm /6000 ./sgidfile ./both ./suidfile #sticky bit for directories - for shared dirs - only people that can remove the file is owner and root chmod +t stickydir #or chmod 1777 stickydir # takes a 1 for sticky dir ls -ld # t or T on end of permissions chmod u+s,g+s,o+t file # sets sgid suid and sticky ","date":"4 July, 2025","id":8,"permalink":"/posts/lfcs-essential-commands-file-management-and-permissions/","summary":"A summary of the commands learnt in the essentials section of the LFCS course preparation.","tags":"LFCS Linux Certificates","title":"LFCS - Essential Commands - File management and permissions"},{"content":"The Linux Foundation Certified System Administrator (LFCS) exam is a 2 hour performance based exam in which you solve multiple issues from the command line.\nWhist I have been using Linux for almost a decade, I would like to formalise my knowledge and make sure all areas are covered. Learning Linux is an essential part of DevOps and SRE roles, so understanding its inner workings and how to manage a Linux system is essential.\nhttps://training.linuxfoundation.org/certification/linux-foundation-certified-sysadmin-lfcs/\nThere are other Linux certifications, namely the Linux Professional Institute LPIC-1 (101 \u0026amp; 102) certifications. These are multiple choice exams, and whilst this is still valuable, I feel the performance based tests will be a better reflection of knowledge and skill. I will likely still study the free LPIC content in future.\nhttps://www.lpi.org/our-certifications/lpic-1-overview/\nThere is also a really valuable YouTube series that will assist in preparation LPIC 1 - Video Series\nThe exam is broken up into the following sections:\nEssential Commands (20%) Operations Deployment (25%) Users and Groups (10%) Networking (25%) Storage(20%) Getting help directly on Linux Whilst information can be gleaned from the web, its an important skill to master being able to source answers locally (ie. on the system you are running on). Here are some of the most useful and common commands.\nMan Pages Man pages provide all the information you need. You can search specific sections depending on the documentation you require.\n# will list the sections man man # examples of getting information ls --help man ls man 1 printf man 3 printf Apropos command The apropos command searches all man pages related to a keyword or phrase. So if you are not sure where you should be searching this is a great tool.\n# searches man pages if you dont know what you need sudo mandb # create apropos db (man database) apropos tar # search for occurences of tar in manpages apropos -s 1,3 tar # sections Finding all executables (man section 1) with the term archiv\nhomelab on  master took 3s ⬢ [Docker] ❯ apropos -s 1 archiv apt-ftparchive (1) - Utility to generate index files ar (1) - create, modify, and extract from archives dpkg-deb (1) - Debian package archive (.deb) manipulation tool dpkg-split (1) - Debian package archive split/join tool funzip (1) - filter for extracting from a ZIP archive in a pipe git-archive (1) - Create an archive of files from a named tree git-bundle (1) - Move objects and refs by archive git-diagnose (1) - Generate a zip archive of diagnostic information git-get-tar-commit-id (1) - Extract commit ID from an archive created using git-archive git-index-pack (1) - Build pack index file for an existing packed archive git-pack-objects (1) - Create a packed archive of objects git-show-index (1) - Show packed archive index git-unpack-objects (1) - Unpack objects from a packed archive git-upload-archive (1) - Send archive back to git-archive git-verify-pack (1) - Validate packed Git archive files gp-archive (1) - archive gprofng experiment data gpg-zip (1) - encrypt or sign files into an archive gpgtar (1) - Encrypt or sign files into an archive ptardiff (1) - program that diffs an extracted archive against an unextracted one ptargrep (1) - Apply pattern matching to the contents of files in a tar archive ranlib (1) - generate an index to an archive tar (1) - an archiving utility tarcat (1) - concatenates the pieces of a GNU tar multi-volume archive unzip (1) - list, test and extract compressed files in a ZIP archive unzipsfx (1) - self-extracting stub for prepending to ZIP archives x86_64-linux-gnu-ar (1) - create, modify, and extract from archives x86_64-linux-gnu-gp-archive (1) - archive gprofng experiment data x86_64-linux-gnu-ranlib (1) - generate an index to an archive zip (1) - package and compress (archive) files zipgrep (1) - search files in a ZIP archive for lines matching a pattern zipinfo (1) - list detailed information about a ZIP archive Tldr-pages Man pages can sometimes be hard to digest, a simpler tool is tldr-pages, which is a collection of community-maintained help pages - which gives examples of how the tool is used.\nsudo apt install tldr tldr tar homelab on  master took 3s ⬢ [Docker] ❯ tldr tar Downloading tldr pages to /home/vscode/.local/share/tldr tar Archiving utility. Often combined with a compression method, such as gzip or bzip2. More information: https://www.gnu.org/software/tar. - [c]reate an archive and write it to a [f]ile: tar cf path/to/target.tar path/to/file1 path/to/file2 ... - [c]reate a g[z]ipped archive and write it to a [f]ile: tar czf path/to/target.tar.gz path/to/file1 path/to/file2 ... - [c]reate a g[z]ipped (compressed) archive from a directory using relative paths: tar czf path/to/target.tar.gz --directory=path/to/directory . - E[x]tract a (compressed) archive [f]ile into the current directory [v]erbosely: tar xvf path/to/source.tar[.gz|.bz2|.xz] - E[x]tract a (compressed) archive [f]ile into the target directory: tar xf path/to/source.tar[.gz|.bz2|.xz] --directory=path/to/directory - [c]reate a compressed archive and write it to a [f]ile, using the file extension to [a]utomatically determine the compression program: tar caf path/to/target.tar.xz path/to/file1 path/to/file2 ... - Lis[t] the contents of a tar [f]ile [v]erbosely: tar tvf path/to/source.tar https://github.com/tldr-pages/tldr\n","date":"1 July, 2025","id":9,"permalink":"/posts/preparation-for-the-lfcs-exam/","summary":"The Linux Foundation Certified System Administrator (LFCS) exam is a 2 hour performance based exam in which you solve multiple issues from the command line.","tags":"LFCS Linux Certificates","title":"Preparation for the LFCS exam"},{"content":"If we update our container registry with a new image, or a 3rd party app in our cluster needs updating\u0026hellip; that\u0026rsquo;s something to automate!\nWe will be using Renovate to automate the pulling in of new images. It will create a merge request in GitHub, and once we approve the MR, the image will be installed into our cluster!\nhttps://github.com/renovatebot/renovate\nOur project structure will look like this:\n├── apps │ ├── base │ ├── production │ └── staging ├── monitoring │ ├── config │ └── controllers │ ├── base │ └── kube-prometheus-stack │ └── staging │ └── kube-prometheus-stack ├── infrastructure │ └── controllers │ ├── base │ └── renovate │ └── staging │ └── renovate First make a new GitHub classic token with repo access, export it and store in a password manager.\nexport RENOVATE_TOKEN=ghp_123456789abcdefghijklmnopq kubectl create secret generic renovate-container-env \\ --from-literal=RENOVATE_TOKEN=ghp_123456789abcdefghijklmnopq \\ --dry-run=client \\ -o yaml \u0026gt; renovate-container-env.yaml Move renovate-container-env.yaml to base/renovate, and encrypt with sops.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place renovate-container-env.yaml Setup your other manifests:\n# infra/controllers/base/renovate/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: renovate # infra/controllers/base/renovate/cronjob.yaml apiVersion: batch/v1 kind: CronJob metadata: name: renovate namespace: renovate spec: schedule: \u0026#34;@hourly\u0026#34; concurrencyPolicy: Forbid jobTemplate: spec: template: spec: containers: - name: renovate image: renovate/renovate:latest args: - m4ttbr1tt/homelab envFrom: - secretRef: name: renovate-container-env - configMapRef: name: renovate-configmap restartPolicy: Never # infra/controllers/base/renovate/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: renovate-configmap namespace: renovate data: RENOVATE_AUTODISCOVER: \u0026#34;false\u0026#34; RENOVATE_GIT_AUTHOR: \u0026#34;Renovate Bot \u0026lt;bot@renovateapp.com\u0026gt;\u0026#34; RENOVATE_PLATFORM: \u0026#34;github\u0026#34; Add this renovate.json into your homelab git root:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://docs.renovatebot.com/renovate-schema.json\u0026#34;, \u0026#34;kubernetes\u0026#34;: { \u0026#34;fileMatch\u0026#34;: [ \u0026#34;\\\\.yaml$\u0026#34; ] } } Push and reconcile with flux.\nhttps://github.com/m4ttbr1tt/homelab\n","date":"30 June, 2025","id":10,"permalink":"/posts/automatic-image-updates-with-renovate/","summary":"If we update our container registry with a new image, or a 3rd party app in our cluster needs updating\u0026hellip; that\u0026rsquo;s something to automate!","tags":"homelab k3s GitOps","title":"Automatic image updates with Renovate"},{"content":"In the last post we port forwarded to the Grafana service, but that kinda sucks.\nI\u0026rsquo;m going to setup an ingress using the helm chart.\nFirst we determine the config for the helm chart by showing its values.\n# need to add repo to helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm show values prometheus-community/kube-prometheus-stack And then adding the necessary config to release.yaml values section.\ningress: enabled: true Force Flux reconcile\u0026hellip;\nflux reconcile kustomization monitoring-controllers --with-source Setup local dns\u0026hellip;\nsudo vim /etc/hosts 192.168.10.60 grafana.mattbritt.com We will now have internal network access to our dashboard!\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":11,"permalink":"/posts/ingress-with-traefik/","summary":"In the last post we port forwarded to the Grafana service, but that kinda sucks.","tags":"homelab k3s GitOps","title":"Ingress with Traefix"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n├── apps │ ├── base │ ├── production │ └── staging ├── monitoring │ ├── config │ └── controllers │ ├── base │ └── kube-prometheus-stack │ └── staging │ └── kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":12,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! 💥\n","date":"25 June, 2025","id":13,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":14,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! 💥\n","date":"18 June, 2025","id":15,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n├── apps │ ├── base │ ├── production │ └── staging ├── infrastructure │ ├── base │ ├── production │ └── staging └── clusters ├── production └── staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n├── apps │ ├── base │ ├── production │ └── staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time 😍\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":16,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful 😍.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":17,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! 😍\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":18,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while 😀\n","date":"7 June, 2025","id":19,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":20,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":21,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS… and I’m done. I’m moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut…\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI’m now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it’s time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":22,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS… and I’m done.  I’m moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":23,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":24,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I’m Matt Britt. I live in Cape Town, South Africa 🇿🇦😍. I’ve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I’ve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I’ve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I’ve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I’m learning, reflect on where I’ve been, and share thoughts on the technologies and practices that matter to me.\nI’ve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest 🤩\n","date":"12 May, 2025","id":25,"permalink":"/about/","summary":"Howzit, I’m Matt Britt. I live in Cape Town, South Africa 🇿🇦😍. I’ve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":26,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn’t mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they’re built into most editors via extensions.\nYou don’t need “full Vim”. Use VSCode with the Vim plugin. That’s more than enough to get the benefits.\nHere’s why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it’s essential\nThe learning curve can be steep. But it doesn’t have to be. Start with just a few motions. Use them until they’re second nature. Then add more.\nVim is a lifelong multiplier. There’s no excuse not to start.\n","date":"25 April, 2019","id":27,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn’t a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn’t know the layout, and more importantly, the keys didn’t match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":28,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn’t a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"}]