[{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":0,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":1,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":2,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":3,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":4,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":5,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":6,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":7,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":8,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":9,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":10,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":11,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":12,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":13,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":14,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":0,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/managing-homelab-secrets/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy the cluster and recreate in all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\n","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy the cluster and recreate in all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\n","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\n","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey Public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-28T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1zd5n9zx0dsdwdggjuvz32ppngn45gk0tcnwlwfs53ev7szefmdfqarudsl ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\n# to gen secret (that will need to be encrypted still) kubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=59c97568-9eda-4dbf-9857-b6cf9cf91259.json --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\n# to gen secret (that will need to be encrypted still) kubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":0,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":1,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":2,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":3,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":4,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":5,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":6,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":7,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":8,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":9,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":10,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":11,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":12,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":13,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":14,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nKubernetes secrets are NOT secret. We have to enrcypt the content\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nKubernetes secrets are NOT secret. We have to enrcypt the content still\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml \u0026gt; Kubernetes secrets are NOT secret. We have to enrcypt the content still ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place test-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1wte6w7qp22pweadzkjv6fwtmdgxycu38ezjydz6rs4an3cc2a4yqkjmvjh enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1wte6w7qp22pweadzkjv6fwtmdgxycu38ezjydz6rs4an3cc2a4yqkjmvjh enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still. The belowe command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1wte6w7qp22pweadzkjv6fwtmdgxycu38ezjydz6rs4an3cc2a4yqkjmvjh enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1wte6w7qp22pweadzkjv6fwtmdgxycu38ezjydz6rs4an3cc2a4yqkjmvjh enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1wte6w7qp22pweadzkjv6fwtmdgxycu38ezjydz6rs4an3cc2a4yqkjmvjh enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\ndecryption: provider: sops secretRef: name: sops-age ","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":1,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":2,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":3,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":4,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":5,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":6,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":7,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":8,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":9,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":10,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":11,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":12,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":13,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":14,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":15,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":16,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"27 June, 2025","id":0,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":1,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":2,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":3,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":4,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":5,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":6,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":7,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":8,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":9,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":10,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":11,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":12,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":13,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":14,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":15,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":16,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"27 June, 2025","id":0,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":1,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":2,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":3,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":4,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":5,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":6,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":7,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":8,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":9,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":10,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":11,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":12,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":13,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":14,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":15,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":16,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"27 June, 2025","id":0,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":1,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":2,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":3,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":4,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":5,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":6,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":7,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":8,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":9,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":10,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":11,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":12,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":13,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":14,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":15,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":16,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"","date":"27 June, 2025","id":0,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":1,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":2,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":3,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":4,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":5,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":6,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":7,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":8,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":9,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":10,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":11,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":12,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":13,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":14,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":15,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":16,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Now if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":0,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":1,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":2,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":3,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":4,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":5,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":6,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":7,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":8,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":9,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":10,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":11,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":12,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":13,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":14,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":15,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":16,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":0,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":1,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":2,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":3,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":4,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":5,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":6,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":7,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":8,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":9,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":10,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":11,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":12,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":13,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":14,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":15,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":0,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":1,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":2,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":3,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":4,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":5,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":6,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":7,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":8,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":9,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":10,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":11,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":12,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":13,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":14,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":15,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":16,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":0,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":1,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":2,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":3,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":4,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":5,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":6,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":7,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":8,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":9,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":10,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":11,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":12,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":13,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":14,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":15,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":16,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":0,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":1,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":2,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":3,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":4,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":5,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":6,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":7,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":8,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":9,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":10,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":11,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":12,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":13,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":14,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":15,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":16,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":0,"permalink":"/posts/ingress-with-traefik/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":1,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":2,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":3,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":4,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":5,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":6,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":7,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":8,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":9,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":10,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":11,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":12,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":13,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":14,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":15,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":16,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":17,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":0,"permalink":"/posts/ingress-with-traefik/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":1,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":2,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":3,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":4,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":5,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":6,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":7,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":8,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":9,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":10,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":11,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":12,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":13,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":14,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":15,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":16,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":17,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we port forwarded to the Grafana service, but that kinda sucks.\nI\u0026rsquo;m going to setup an ingress using the helm chart.\nFirst we determine the config for the helm chart by showing its values.\n# need to add repo to helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm show values prometheus-community/kube-prometheus-stack And then adding the necessary config to release.yaml values section.\ningress: enabled:true name: traefik hosts: - grafana.mattbritt.com Force Flux reconcile\u0026hellip;\nflux reconcile kustomization monitoring-controllers --with-source Setup local dns\u0026hellip;\nsudo vim /etc/hosts 192.168.10.60 grafana.mattbritt.com We will now have internal network access to our dashboard!\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":0,"permalink":"/posts/ingress-with-traefik/","summary":"In the last post we port forwarded to the Grafana service, but that kinda sucks.","tags":"homelab k3s GitOps","title":"Ingress with Traefix"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":1,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":2,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":3,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":4,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":5,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":6,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":7,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":8,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":9,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":10,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":11,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":12,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":13,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":14,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":15,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":16,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":17,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we port forwarded to the Grafana service, but that kinda sucks.\nI\u0026rsquo;m going to setup an ingress using the helm chart.\nFirst we determine the config for the helm chart by showing its values.\n# need to add repo to helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm show values prometheus-community/kube-prometheus-stack And then adding the necessary config to release.yaml values section.\ningress: enabled:true Force Flux reconcile\u0026hellip;\nflux reconcile kustomization monitoring-controllers --with-source Setup local dns\u0026hellip;\nsudo vim /etc/hosts 192.168.10.60 grafana.mattbritt.com We will now have internal network access to our dashboard!\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":0,"permalink":"/posts/ingress-with-traefik/","summary":"In the last post we port forwarded to the Grafana service, but that kinda sucks.","tags":"homelab k3s GitOps","title":"Ingress with Traefix"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":1,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":2,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":3,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":4,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":5,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":6,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":7,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":8,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":9,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":10,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":11,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":12,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":13,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":14,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":15,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":16,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":17,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we port forwarded to the Grafana service, but that kinda sucks.\nI\u0026rsquo;m going to setup an ingress using the helm chart.\nFirst we determine the config for the helm chart by showing its values.\n# need to add repo to helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm show values prometheus-community/kube-prometheus-stack And then adding the necessary config to release.yaml values section.\ningress: enabled: true Force Flux reconcile\u0026hellip;\nflux reconcile kustomization monitoring-controllers --with-source Setup local dns\u0026hellip;\nsudo vim /etc/hosts 192.168.10.60 grafana.mattbritt.com We will now have internal network access to our dashboard!\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":0,"permalink":"/posts/ingress-with-traefik/","summary":"In the last post we port forwarded to the Grafana service, but that kinda sucks.","tags":"homelab k3s GitOps","title":"Ingress with Traefix"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":1,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":2,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":3,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":4,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":5,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":6,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":7,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":8,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":9,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":10,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":11,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":12,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":13,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":14,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":15,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":16,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":17,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we port forwarded to the Grafana service, but that kinda sucks.\nI\u0026rsquo;m going to setup an ingress using the helm chart.\nFirst we determine the config for the helm chart by showing its values.\n# need to add repo to helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm show values prometheus-community/kube-prometheus-stack And then adding the necessary config to release.yaml values section.\ningress: enabled: true Force Flux reconcile\u0026hellip;\nflux reconcile kustomization monitoring-controllers --with-source Setup local dns\u0026hellip;\nsudo vim /etc/hosts 192.168.10.60 grafana.mattbritt.com We will now have internal network access to our dashboard!\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":0,"permalink":"/posts/ingress-with-traefik/","summary":"In the last post we port forwarded to the Grafana service, but that kinda sucks.","tags":"homelab k3s GitOps","title":"Ingress with Traefix"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":1,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":2,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":3,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":4,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":5,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":6,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":7,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":8,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":9,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":10,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":11,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":12,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":13,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":14,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":15,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":16,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":17,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we port forwarded to the Grafana service, but that kinda sucks.\nI\u0026rsquo;m going to setup an ingress using the helm chart.\nFirst we determine the config for the helm chart by showing its values.\n# need to add repo to helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm show values prometheus-community/kube-prometheus-stack And then adding the necessary config to release.yaml values section.\ningress: enabled: true Force Flux reconcile\u0026hellip;\nflux reconcile kustomization monitoring-controllers --with-source Setup local dns\u0026hellip;\nsudo vim /etc/hosts 192.168.10.60 grafana.mattbritt.com We will now have internal network access to our dashboard!\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":0,"permalink":"/posts/ingress-with-traefik/","summary":"In the last post we port forwarded to the Grafana service, but that kinda sucks.","tags":"homelab k3s GitOps","title":"Ingress with Traefix"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":1,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":2,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":3,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":4,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":5,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":6,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":7,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":8,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":9,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":10,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":11,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":12,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":13,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":14,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":15,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":16,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":17,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we port forwarded to the Grafana service, but that kinda sucks.\nI\u0026rsquo;m going to setup an ingress using the helm chart.\nFirst we determine the config for the helm chart by showing its values.\n# need to add repo to helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm show values prometheus-community/kube-prometheus-stack And then adding the necessary config to release.yaml values section.\ningress: enabled: true Force Flux reconcile\u0026hellip;\nflux reconcile kustomization monitoring-controllers --with-source Setup local dns\u0026hellip;\nsudo vim /etc/hosts 192.168.10.60 grafana.mattbritt.com We will now have internal network access to our dashboard!\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":0,"permalink":"/posts/automatic-image-updates-with-renovate/","summary":"In the last post we port forwarded to the Grafana service, but that kinda sucks.","tags":"homelab k3s GitOps","title":"Ingress with Traefix"},{"content":"In the last post we port forwarded to the Grafana service, but that kinda sucks.\nI\u0026rsquo;m going to setup an ingress using the helm chart.\nFirst we determine the config for the helm chart by showing its values.\n# need to add repo to helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm show values prometheus-community/kube-prometheus-stack And then adding the necessary config to release.yaml values section.\ningress: enabled: true Force Flux reconcile\u0026hellip;\nflux reconcile kustomization monitoring-controllers --with-source Setup local dns\u0026hellip;\nsudo vim /etc/hosts 192.168.10.60 grafana.mattbritt.com We will now have internal network access to our dashboard!\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":1,"permalink":"/posts/ingress-with-traefik/","summary":"In the last post we port forwarded to the Grafana service, but that kinda sucks.","tags":"homelab k3s GitOps","title":"Ingress with Traefix"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":2,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":3,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":4,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":5,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":6,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":7,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":8,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":9,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":10,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":11,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":12,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":13,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":14,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":15,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":16,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":17,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":18,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"In the last post we port forwarded to the Grafana service, but that kinda sucks.\nI\u0026rsquo;m going to setup an ingress using the helm chart.\nFirst we determine the config for the helm chart by showing its values.\n# need to add repo to helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm show values prometheus-community/kube-prometheus-stack And then adding the necessary config to release.yaml values section.\ningress: enabled: true Force Flux reconcile\u0026hellip;\nflux reconcile kustomization monitoring-controllers --with-source Setup local dns\u0026hellip;\nsudo vim /etc/hosts 192.168.10.60 grafana.mattbritt.com We will now have internal network access to our dashboard!\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":0,"permalink":"/posts/automatic-image-updates-with-renovate/","summary":"In the last post we port forwarded to the Grafana service, but that kinda sucks.","tags":"homelab k3s GitOps","title":"Automatic image updates with Renovate"},{"content":"In the last post we port forwarded to the Grafana service, but that kinda sucks.\nI\u0026rsquo;m going to setup an ingress using the helm chart.\nFirst we determine the config for the helm chart by showing its values.\n# need to add repo to helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm show values prometheus-community/kube-prometheus-stack And then adding the necessary config to release.yaml values section.\ningress: enabled: true Force Flux reconcile\u0026hellip;\nflux reconcile kustomization monitoring-controllers --with-source Setup local dns\u0026hellip;\nsudo vim /etc/hosts 192.168.10.60 grafana.mattbritt.com We will now have internal network access to our dashboard!\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":1,"permalink":"/posts/ingress-with-traefik/","summary":"In the last post we port forwarded to the Grafana service, but that kinda sucks.","tags":"homelab k3s GitOps","title":"Ingress with Traefix"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":2,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":3,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":4,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":5,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":6,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":7,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":8,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":9,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":10,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":11,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":12,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":13,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":14,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":15,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":16,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":17,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":18,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"If we update our container registry with a new image, or the 3rd party apps in our cluster need updating\u0026hellip; that\u0026rsquo;s something to automate!\nWe will be using renovate to automate the pulling in of new images. It will create an merge request in github, and once we approve the image will be installed into our cluster!\nhttps://github.com/renovatebot/renovate\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ renovate ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ renovate First make a new github classic token with repo access and store in a password manager.\nkubectl create secret generic renovate-container-env \\ --from-literal=RENOVATE_TOKEN=ghp_123456789abcdefghijklmnopqrstuvwxyz \\ --dry-run=client \\ -o yaml \u0026gt; renovate-container-env.yaml Move renovate-container-env.yaml to base/renovate, and encrypt with sops.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place renovate-container-env.yaml Setup other manifests:\n# infra/controllers/base/renovate/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: renovate # infra/controllers/base/renovate/cronjob.yaml apiVersion: batch/v1 kind: CronJob metadata: name: renovate namespace: renovate spec: schedule: \u0026#34;@hourly\u0026#34; concurrencyPolicy: Forbid jobTemplate: spec: template: spec: containers: - name: renovate image: renovate/renovate:latest args: - m4ttbr1tt/homelab envFrom: - secretRef: name: renovate-container-env - configMapRef: name: renovate-configmap restartPolicy: Never # infra/controllers/base/renovate/configmap.yaml # settings that arent secret apiVersion: v1 kind: ConfigMap metadata: name: renovate-configmap namespace: renovate data: RENOVATE_AUTODISCOVER: \u0026#34;false\u0026#34; RENOVATE_GIT_AUTHOR: \u0026#34;Renovate Bot \u0026lt;bot@renovateapp.com\u0026gt;\u0026#34; RENOVATE_PLATFORM: \u0026#34;github\u0026#34; Add this renovate.json into your homelab root:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://docs.renovatebot.com/renovate-schema.json\u0026#34;, \u0026#34;kubernetes\u0026#34;: { \u0026#34;fileMatch\u0026#34;: [ \u0026#34;\\\\.yaml$\u0026#34; ] } } Push and reconcile with flux.\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":0,"permalink":"/posts/automatic-image-updates-with-renovate/","summary":"If we update our container registry with a new image, or the 3rd party apps in our cluster need updating\u0026hellip; that\u0026rsquo;s something to automate!","tags":"homelab k3s GitOps","title":"Automatic image updates with Renovate"},{"content":"In the last post we port forwarded to the Grafana service, but that kinda sucks.\nI\u0026rsquo;m going to setup an ingress using the helm chart.\nFirst we determine the config for the helm chart by showing its values.\n# need to add repo to helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm show values prometheus-community/kube-prometheus-stack And then adding the necessary config to release.yaml values section.\ningress: enabled: true Force Flux reconcile\u0026hellip;\nflux reconcile kustomization monitoring-controllers --with-source Setup local dns\u0026hellip;\nsudo vim /etc/hosts 192.168.10.60 grafana.mattbritt.com We will now have internal network access to our dashboard!\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":1,"permalink":"/posts/ingress-with-traefik/","summary":"In the last post we port forwarded to the Grafana service, but that kinda sucks.","tags":"homelab k3s GitOps","title":"Ingress with Traefix"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":2,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":3,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":4,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":5,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":6,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":7,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":8,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":9,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":10,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":11,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":12,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":13,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":14,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":15,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":16,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":17,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":18,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"If we update our container registry with a new image, or a 3rd party apps in our cluster need updating\u0026hellip; that\u0026rsquo;s something to automate!\nWe will be using renovate to automate the pulling in of new images. It will create an merge request in github, and once we approve the image will be installed into our cluster!\nhttps://github.com/renovatebot/renovate\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ renovate ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ renovate First make a new github classic token with repo access and store in a password manager.\nkubectl create secret generic renovate-container-env \\ --from-literal=RENOVATE_TOKEN=ghp_123456789abcdefghijklmnopqrstuvwxyz \\ --dry-run=client \\ -o yaml \u0026gt; renovate-container-env.yaml Move renovate-container-env.yaml to base/renovate, and encrypt with sops.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place renovate-container-env.yaml Setup other manifests:\n# infra/controllers/base/renovate/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: renovate # infra/controllers/base/renovate/cronjob.yaml apiVersion: batch/v1 kind: CronJob metadata: name: renovate namespace: renovate spec: schedule: \u0026#34;@hourly\u0026#34; concurrencyPolicy: Forbid jobTemplate: spec: template: spec: containers: - name: renovate image: renovate/renovate:latest args: - m4ttbr1tt/homelab envFrom: - secretRef: name: renovate-container-env - configMapRef: name: renovate-configmap restartPolicy: Never # infra/controllers/base/renovate/configmap.yaml # settings that arent secret apiVersion: v1 kind: ConfigMap metadata: name: renovate-configmap namespace: renovate data: RENOVATE_AUTODISCOVER: \u0026#34;false\u0026#34; RENOVATE_GIT_AUTHOR: \u0026#34;Renovate Bot \u0026lt;bot@renovateapp.com\u0026gt;\u0026#34; RENOVATE_PLATFORM: \u0026#34;github\u0026#34; Add this renovate.json into your homelab root:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://docs.renovatebot.com/renovate-schema.json\u0026#34;, \u0026#34;kubernetes\u0026#34;: { \u0026#34;fileMatch\u0026#34;: [ \u0026#34;\\\\.yaml$\u0026#34; ] } } Push and reconcile with flux.\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":0,"permalink":"/posts/automatic-image-updates-with-renovate/","summary":"If we update our container registry with a new image, or a 3rd party apps in our cluster need updating\u0026hellip; that\u0026rsquo;s something to automate!","tags":"homelab k3s GitOps","title":"Automatic image updates with Renovate"},{"content":"In the last post we port forwarded to the Grafana service, but that kinda sucks.\nI\u0026rsquo;m going to setup an ingress using the helm chart.\nFirst we determine the config for the helm chart by showing its values.\n# need to add repo to helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm show values prometheus-community/kube-prometheus-stack And then adding the necessary config to release.yaml values section.\ningress: enabled: true Force Flux reconcile\u0026hellip;\nflux reconcile kustomization monitoring-controllers --with-source Setup local dns\u0026hellip;\nsudo vim /etc/hosts 192.168.10.60 grafana.mattbritt.com We will now have internal network access to our dashboard!\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":1,"permalink":"/posts/ingress-with-traefik/","summary":"In the last post we port forwarded to the Grafana service, but that kinda sucks.","tags":"homelab k3s GitOps","title":"Ingress with Traefix"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":2,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":3,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":4,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":5,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":6,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":7,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":8,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":9,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":10,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":11,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":12,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":13,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":14,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":15,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":16,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":17,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":18,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"If we update our container registry with a new image, or a 3rd party app in our cluster needs updating\u0026hellip; that\u0026rsquo;s something to automate!\nWe will be using Renovate to automate the pulling in of new images. It will create a merge request in GitHub, and once we approve the MR, the image will be installed into our cluster!\nhttps://github.com/renovatebot/renovate\nOur project structure will look like this:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ renovate ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ renovate First make a new GitHub classic token with repo access, export it and store in a password manager.\nexport RENOVATE_TOKEN=ghp_123456789abcdefghijklmnopq kubectl create secret generic renovate-container-env \\ --from-literal=RENOVATE_TOKEN=ghp_123456789abcdefghijklmnop \\ --dry-run=client \\ -o yaml \u0026gt; renovate-container-env.yaml Move renovate-container-env.yaml to base/renovate, and encrypt with sops.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place renovate-container-env.yaml Setup your other manifests:\n# infra/controllers/base/renovate/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: renovate # infra/controllers/base/renovate/cronjob.yaml apiVersion: batch/v1 kind: CronJob metadata: name: renovate namespace: renovate spec: schedule: \u0026#34;@hourly\u0026#34; concurrencyPolicy: Forbid jobTemplate: spec: template: spec: containers: - name: renovate image: renovate/renovate:latest args: - m4ttbr1tt/homelab envFrom: - secretRef: name: renovate-container-env - configMapRef: name: renovate-configmap restartPolicy: Never # infra/controllers/base/renovate/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: renovate-configmap namespace: renovate data: RENOVATE_AUTODISCOVER: \u0026#34;false\u0026#34; RENOVATE_GIT_AUTHOR: \u0026#34;Renovate Bot \u0026lt;bot@renovateapp.com\u0026gt;\u0026#34; RENOVATE_PLATFORM: \u0026#34;github\u0026#34; Add this renovate.json into your homelab git root:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://docs.renovatebot.com/renovate-schema.json\u0026#34;, \u0026#34;kubernetes\u0026#34;: { \u0026#34;fileMatch\u0026#34;: [ \u0026#34;\\\\.yaml$\u0026#34; ] } } Push and reconcile with flux.\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":0,"permalink":"/posts/automatic-image-updates-with-renovate/","summary":"If we update our container registry with a new image, or a 3rd party app in our cluster needs updating\u0026hellip; that\u0026rsquo;s something to automate!","tags":"homelab k3s GitOps","title":"Automatic image updates with Renovate"},{"content":"In the last post we port forwarded to the Grafana service, but that kinda sucks.\nI\u0026rsquo;m going to setup an ingress using the helm chart.\nFirst we determine the config for the helm chart by showing its values.\n# need to add repo to helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm show values prometheus-community/kube-prometheus-stack And then adding the necessary config to release.yaml values section.\ningress: enabled: true Force Flux reconcile\u0026hellip;\nflux reconcile kustomization monitoring-controllers --with-source Setup local dns\u0026hellip;\nsudo vim /etc/hosts 192.168.10.60 grafana.mattbritt.com We will now have internal network access to our dashboard!\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":1,"permalink":"/posts/ingress-with-traefik/","summary":"In the last post we port forwarded to the Grafana service, but that kinda sucks.","tags":"homelab k3s GitOps","title":"Ingress with Traefix"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":2,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":3,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":4,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":5,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":6,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":7,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":8,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":9,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":10,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":11,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":12,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":13,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":14,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":15,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":16,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":17,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":18,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"},{"content":"If we update our container registry with a new image, or a 3rd party app in our cluster needs updating\u0026hellip; that\u0026rsquo;s something to automate!\nWe will be using Renovate to automate the pulling in of new images. It will create a merge request in GitHub, and once we approve the MR, the image will be installed into our cluster!\nhttps://github.com/renovatebot/renovate\nOur project structure will look like this:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ renovate ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ renovate First make a new GitHub classic token with repo access, export it and store in a password manager.\nexport RENOVATE_TOKEN=ghp_123456789abcdefghijklmnopq kubectl create secret generic renovate-container-env \\ --from-literal=RENOVATE_TOKEN=ghp_123456789abcdefghijklmnopq \\ --dry-run=client \\ -o yaml \u0026gt; renovate-container-env.yaml Move renovate-container-env.yaml to base/renovate, and encrypt with sops.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place renovate-container-env.yaml Setup your other manifests:\n# infra/controllers/base/renovate/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: renovate # infra/controllers/base/renovate/cronjob.yaml apiVersion: batch/v1 kind: CronJob metadata: name: renovate namespace: renovate spec: schedule: \u0026#34;@hourly\u0026#34; concurrencyPolicy: Forbid jobTemplate: spec: template: spec: containers: - name: renovate image: renovate/renovate:latest args: - m4ttbr1tt/homelab envFrom: - secretRef: name: renovate-container-env - configMapRef: name: renovate-configmap restartPolicy: Never # infra/controllers/base/renovate/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: renovate-configmap namespace: renovate data: RENOVATE_AUTODISCOVER: \u0026#34;false\u0026#34; RENOVATE_GIT_AUTHOR: \u0026#34;Renovate Bot \u0026lt;bot@renovateapp.com\u0026gt;\u0026#34; RENOVATE_PLATFORM: \u0026#34;github\u0026#34; Add this renovate.json into your homelab git root:\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://docs.renovatebot.com/renovate-schema.json\u0026#34;, \u0026#34;kubernetes\u0026#34;: { \u0026#34;fileMatch\u0026#34;: [ \u0026#34;\\\\.yaml$\u0026#34; ] } } Push and reconcile with flux.\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":0,"permalink":"/posts/automatic-image-updates-with-renovate/","summary":"If we update our container registry with a new image, or a 3rd party app in our cluster needs updating\u0026hellip; that\u0026rsquo;s something to automate!","tags":"homelab k3s GitOps","title":"Automatic image updates with Renovate"},{"content":"In the last post we port forwarded to the Grafana service, but that kinda sucks.\nI\u0026rsquo;m going to setup an ingress using the helm chart.\nFirst we determine the config for the helm chart by showing its values.\n# need to add repo to helm helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm show values prometheus-community/kube-prometheus-stack And then adding the necessary config to release.yaml values section.\ningress: enabled: true Force Flux reconcile\u0026hellip;\nflux reconcile kustomization monitoring-controllers --with-source Setup local dns\u0026hellip;\nsudo vim /etc/hosts 192.168.10.60 grafana.mattbritt.com We will now have internal network access to our dashboard!\nhttps://github.com/m4ttbr1tt/homelab\n","date":"28 June, 2025","id":1,"permalink":"/posts/ingress-with-traefik/","summary":"In the last post we port forwarded to the Grafana service, but that kinda sucks.","tags":"homelab k3s GitOps","title":"Ingress with Traefix"},{"content":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.\nPrometheus collects and stores metrics, Grafana turns them into insights and fancy dashboards.\nOur repo will end up with the following directory structure:\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ monitoring ‚îÇ ‚îú‚îÄ‚îÄ config ‚îÇ ‚îî‚îÄ‚îÄ controllers ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îÇ ‚îî‚îÄ‚îÄ kube-prometheus-stack Here are the manifests (under monitoring/controllers) that we need to get this running:\n# /base/k-p-s/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # /base/k-p-s/repository.yaml apiVersion: source.toolkit.fluxcd.io/v1 kind: HelmRepository metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 24h url: https://prometheus-community.github.io/helm-charts # /base/k-p-s/release.yaml apiVersion: helm.toolkit.fluxcd.io/v2 kind: HelmRelease metadata: name: kube-prometheus-stack namespace: monitoring spec: interval: 30m chart: spec: chart: kube-prometheus-stack version: \u0026#34;66.2.2\u0026#34; sourceRef: kind: HelmRepository name: kube-prometheus-stack namespace: monitoring interval: 12h install: crds: Create upgrade: crds: CreateReplace driftDetection: mode: enabled ignore: # Ignore \u0026#34;validated\u0026#34; annotation which is not inserted during install - paths: [\u0026#34;/metadata/annotations/prometheus-operator-validated\u0026#34;] target: kind: PrometheusRule values: grafana: adminPassword: somepassword # todo change # /base/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - repository.yaml - release.yaml # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: monitoring resources: - ../../base/kube-prometheus-stack/ Next under clusters/staging copy the apps.yaml and edit to the following:\n# clusters/staging/monitoring.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: monitoring-controllers namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./monitoring/controllers/staging prune: true # /staging/k-p-s/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - kube-prometheus-stack Push to git and flux will reconcile and deploy the resources.\nNow if we port forward to the service we will have access to our Grafana dashboard:\nkubectl -n monitoring port-forward svc/kube-prometheus-stack-grafana 8080:80 https://github.com/m4ttbr1tt/homelab\n","date":"27 June, 2025","id":2,"permalink":"/posts/monitoring-with-prometheus-and-grafana/","summary":"Visibility and monitoring of our homelab will be done by installing a helm chart called kube-prometheus-stack.","tags":"homelab k3s GitOps","title":"Monitoring with Prometheus \u0026 Grafana"},{"content":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;\nI want to be able to destroy and recreate the cluster all from one repository, so the secrets need to be stored in git, and thus we will need to store the secrets in a secure way.\nI am using sops (with age encryption) https://github.com/getsops/sops\nFirst setup sops and age, for me thats\u0026hellip;\nsudo pacman -S sops age Then generate a key pair and save to your password manager. (we will be deleting the age.agekey file after we are done)\nage-keygen -o age.agekey cat age.agekey The contents of the file will look something like this\u0026hellip;\nPublic key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv # created: 2025-06-24T10:04:52+02:00 # public key: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv AGE-SECRET-KEY-13HMN4HPFLZ60ECZUHHYT5JW75WNG3AL3JCYJ36H90209PKRRYWHQ0320ZS Make sure you set an environment variable with your public key:\nexport AGE_PUBLIC=age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv Next we will generate a secret manifest that uses our points to our Cloudflare credentials file.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json \\ --dry-run=client -o yaml \u0026gt; cloudflare-secret.yaml Kubernetes secrets are NOT secret. We have to enrcypt the content still.\nThe below command references your env variable with the public key, targets the specifice section of your yaml with regex, and makes an in-place edit of the secret file.\nsops --age=$AGE_PUBLIC \\ --encrypt --encrypted-regex \u0026#39;^(data|stringData)$\u0026#39; --in-place cloudflare-secret.yaml If you cat the file you will now see the contents of the credintials file have been encrypted in place\napiVersion: v1 data: credentials.json: ENC[AES256_GCM,data:aHXoMNZSR1mzMrnWMrpwNVJ65MF3axL43OanZnmdjYKViSphDBl491s+tMhI0G3xeQiFBKcBFsHmKT7c8PaV+WgfJG7yg6J36QtJxLz7xG87d9Z5PkWfSo390tPxSr8GLpbi3Qgr+AVxvwjY3F3E8H19uFEyJQQxWDS/sTSNm/wDIdhpIsuTzqUZvVuZLlJmV4Unj5TaZUk1/divBvivShLLdPh1smFjqdH+r1f5uTb2lr6sM82U5do0A89FOOwM+yd6J2c7P9kGQJ+yMktEW0hF/gJkSYRjsB/8YEAZ+JvhEkE69Njoxtmu4jDSbZSN5Ndr13yhYWNviVZwnMUNa1x/fGv6JoA4A2VVGT65OYY4jZvl/xQBsD8uuq1YvtGz8tmGoZHIjFT8U0MGY8Mu6l5uN8sB44k8LhEHTq2+HC+34o6GqYZX7y5zQWJlrJ2NOW4fFVZCr+U6YvWr1BxCdhX6rk6GOMnZ18zWvY6LNHr+RaUH+AR50fmW+IO19NC2v3spxrSjBHH+oSmdksXVU4FVxLJskwyCbNXfcTVG9OIH3FnB3lilxvvy6b1CA5WA6Z5LXSNUxLEi99IqDjQz5SyY7LclRXOgJxz3kh4Zq0m+RaAMJTaSO4JKPoTpDtfbECYnMxYWeJhrKdOVQLNuOZWIg6S9O6CuxmOOezkq/9M=,iv:2h/BVKyWDyQIRYarBlYYraXjD2hENDyA9HfjMnu5eg0=,tag:bMC3lUKS/9gnX4BJT9Q/qQ==,type:str] kind: Secret metadata: creationTimestamp: null name: tunnel-credentials sops: age: - recipient: age1j6pyg3y73pt5leldl2csjk8a369fa2xxmuanywx6mn0mfhwc4dzs0lvthv enc: | -----BEGIN AGE ENCRYPTED FILE----- YWdlLWVuY3J5cHRpb24ub3JnL3YxCi0+IFgyNTUxOSBESUtsT2dvWkJFdTJCaGgr azNvOXl1NjFCWU1wTi8rb0RLRjhYeFM2QTFBCkdZVitlVUlCNkxvQlBWVnJZUUlG SDQ2R1NZMkZVY0FuVUhpT05nbkc4bUEKLS0tIEwwK3FZN1lrK05pcHhpOGlldVN2 Sk1zdnRTdWd1QmhFZXZRWlZKMHJEQnMKyIMSF5qOq6z7AF/4vTVczlYS0P6mV3q2 WBq/t/UVAmts0TCn8t5xv4M+EXghs1QBkr35A2ClYBYgjd7qQLSZgw== -----END AGE ENCRYPTED FILE----- lastmodified: \u0026#34;2025-06-28T08:23:19Z\u0026#34; mac: ENC[AES256_GCM,data:af++553y8OX+VAy7Ygg0zIYdMdEPUfwI6mSzoCqgTa2/Uqp//o94NwUicjp7ZGUmoX6BM6gK5ULd5k8Du1ncj5K5e2boFeTBk099accVeYtVwiknAWViVw4DD2Mnm9vCmDJJMNacB4Ax6i/Z0+D/MNy/076xUMyq23tqmPK98KA=,iv:LPa9LpP+iJmzrdKqpzysQdXtYHPwoHHuZqLhSIF7LiI=,tag:jU1NX0fo4mlxezAh7CourQ==,type:str] encrypted_regex: ^(data|stringData)$ version: 3.10.2 In order for sops do decrypt our secret it needs the private key (this is a manual step when setting up your cluster)\ncat age.agekey | kubectl create secret generic sops-age \\ --namespace=flux-system \\ --from-file=age.agekey=/dev/stdin The last thing is to amend our apps.yaml file to include sops for decryption.\n# update spec of apps.yaml so decryption works decryption: provider: sops secretRef: name: sops-age Push to git and flux will reconcile and decrypt your secret! üí•\n","date":"25 June, 2025","id":3,"permalink":"/posts/managing-homelab-secrets/","summary":"In the last post we manually created a Cloudflare secret directly using kubectl, which goes against what we are trying to achieve with GitOps\u0026hellip;","tags":"homelab k3s GitOps","title":"Managing homelab secrets"},{"content":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.\nIt would be better, from a security point of view, not to have to open any ports to the internet.\nCloudflare offers a nice free solution (if you host a domain with them) called Cloudflare tunnels.\nThey work by opening a connection from the client (application end) to Cloudflare, that stays open and allows traffic to route from Cloudflare to your network - after you setup a CNAME record.\nFirst we need to install and configure the cloudflared app.\npacman -S cloudflared cloudflared tunnel login We then need to create a tunnel named ldpi.\n# will createe a json file /home/matt/.cloudflared cloudflared tunnel create ldpi We then use the generated file to create a secret in K3S.\nkubectl create secret generic tunnel-credentials \\ --from-file=credentials.json=58c97568-9eda-4dbf-9857-b6cf9cf91259.json Using the application id in the json file we need to add a CNAME record to Cloudflare (with proxying)\n# The CNAME needs to point to this record (our cname is links) 58c97568-9eda-4dbf-0857-b6cf9cf91259.cfargotunnel.com We now need to add a kubernetes service that will be configured to point to our deployment.\n# in service.yaml (base layer of linkding) # and edit kustomization file! apiVersion: v1 kind: Service metadata: name: linkding spec: ports: - port: 9090 selector: app: linkding type: ClusterIP We then create a new deployment that will run the cloudflared image and make the initial call to Cloudflare to setup the tunnel.\napiVersion: apps/v1 kind: Deployment metadata: name: cloudflared spec: selector: matchLabels: app: cloudflared replicas: 2 template: metadata: labels: app: cloudflared spec: containers: - name: cloudflared image: cloudflare/cloudflared:latest args: - tunnel - --config - /etc/cloudflared/config/config.yaml - run livenessProbe: httpGet: path: /ready port: 2000 failureThreshold: 1 initialDelaySeconds: 10 periodSeconds: 10 volumeMounts: - name: config mountPath: /etc/cloudflared/config readOnly: true - name: creds mountPath: /etc/cloudflared/creds readOnly: true volumes: - name: creds secret: secretName: tunnel-credentials - name: config configMap: name: cloudflared items: - key: config.yaml path: config.yaml --- apiVersion: v1 kind: ConfigMap metadata: name: cloudflared data: config.yaml: | tunnel: ldpi credentials-file: /etc/cloudflared/creds/credentials.json ingress: - hostname: links.mattbritt.com service: http://linkding:9090 When we push these changes to our repo, Flux will provision and setup the tunnel, we are live!\n","date":"21 June, 2025","id":4,"permalink":"/posts/exposing-our-application-to-the-internet/","summary":"The linkding app is now running on our cluster, but in order to make it useful it would be good to expose it to the internet.","tags":"homelab k3s GitOps","title":"Exposing our application to the internet"},{"content":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.\nFirst we need to provision a PVC.\n#base/linkding/pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: linkding-pv-claim namespace: linkding spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi And then alter our deployment to add the volume and mount it to the container.\n#base/linkding/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 volumeMounts: - name: linkding-data mountPath: \u0026#34;/etc/linkding/data\u0026#34; volumes: - name: linkding-data persistentVolumeClaim: claimName: linkding-data-pvc With Flux running in our cluster all we need to do is to commit and push our changes to the homelab repo! üí•\n","date":"18 June, 2025","id":5,"permalink":"/posts/adding-storage-to-our-homelab-application/","summary":"In order to persist any data that will be generated be linkding, we need to make sure that we have persistent storage.","tags":"homelab k3s GitOps","title":"Adding storage to our homelab application"},{"content":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.\nhttps://linkding.link/\nWays to structure your git repo Flux has some recommendations on how to structure your git repo. For my homelab I\u0026rsquo;ll be using the below structure.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îú‚îÄ‚îÄ infrastructure ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging ‚îî‚îÄ‚îÄ clusters ‚îú‚îÄ‚îÄ production ‚îî‚îÄ‚îÄ staging For details and alternatives you can see more details here\u0026hellip;\nhttps://fluxcd.io/flux/guides/repository-structure\nKustomization file Flux has a kustomization file that reads in other kustomize files (its the entry point)\nAny Kustomization file in the clusters \u0026gt; staging dir will be included.\nFlux app config file Add this file to clusters \u0026gt; staging (it is configured to look in the ./apps/staging path that we are setting up in the next step)\n# apps.yaml apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: apps namespace: flux-system spec: interval: 1m retryInterval: 1m timeout: 1m sourceRef: kind: GitRepository name: flux-system path: ./apps/staging # references the dir just setup prune: true Base layers Shared config is stored in base layers and then overridden in derived layers with a patches definition.\nEg.\n‚îú‚îÄ‚îÄ apps ‚îÇ ‚îú‚îÄ‚îÄ base ‚îÇ ‚îú‚îÄ‚îÄ production ‚îÇ ‚îî‚îÄ‚îÄ staging Adding an app Kustomization file mkdir -p apps/staging/linkding mkdir -p apps/base/linkding cd apps/staging/linkding touch kustomization.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization namespace: linkding resources: - ../.../base/linkding # reads in base config directory cd apps/base/linkding touch kustomization.yaml touch namespace.yaml touch deployment.yaml # kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - namespace.yaml - deployment.yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: linkding # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: linkding spec: replicas: 1 selector: matchLabels: app: linkding template: metadata: labels: app: linkding spec: containers: - name: linkding image: sissbruecker/linkding:1.31.0 ports: - containerPort: 9090 Once pushed to git you can check the status by running the below command. Once the cluster state has been reconciled, you will see the state change (or you will receive a notification of any failures)\nflux get kustomizations And then port forward to check that the app is running.\nkubectl port-forward linkding-pod 8080:9090 #8080 is localport 9090 is the port exposed by deployment We will be setting up a service to manage this soon. Until next time üòç\nhttps://github.com/m4ttbr1tt/homelab\n","date":"14 June, 2025","id":6,"permalink":"/posts/installing-our-first-homelab-application-with-gitops/","summary":"After setting up Flux on the cluster, its time to put it to use! I\u0026rsquo;ll be deploying an app called Linkding, which is a bookmark manager.","tags":"homelab k3s GitOps","title":"Installing our first homelab application with GitOps"},{"content":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.\nWhat is GitOps? GitOps is a best practice for code delivery of applications and infrastructure. It uses git as the source of truth and a controller that runs on the cluster, constantly checking the git repo for any changes and reconciling the declared state with the cluster.\nGitOps enables declarative cluster state\nWhen I first used Kubernetes in production in 2018, we did not have GitOps. The deployments and manifests were written in bash scripts and it was difficult to maintain, there was often drift or uncertainty as to the state the cluster was in. GitOps is a modern way to resolve this, and its beautiful üòç.\nWhich GitOps tool to choose? There are two main tools in this space Flux and Argo.\nI\u0026rsquo;ve chosen Flux for the following reasons:\nKustomize is used by default (Application customisation) CLI focused (Argo has a great UI, but Id rather not use that) Used by default in Azure AKS (which is what I know) https://fluxcd.io\nInstalling Flux First create a personal token on GitHub and export the token, as well as your GitHub username, as an environment variable.\nexport GITHUB_TOKEN=ghp_234234234234234234234 export GITHUB_USER=m4ttbr1tt Next check your cluster meets the prerequisites\u0026hellip;\nflux check --pre ..and install Flux.\n# will install to cluster and then push manifests to github flux bootstrap github \\ --owner=$GITHUB_USER \\ --repository=homelab \\ --branch=master \\ --path=./clusters/staging \\ --personal You will now see Flux pods on the cluster\u0026hellip; https://github.com/m4ttbr1tt/homelab\n","date":"11 June, 2025","id":7,"permalink":"/posts/homelab-gitops-with-flux/","summary":"Today I installed Flux onto the cluster in order to implement GitOps for my homelab.","tags":"homelab k3s GitOps","title":"GitOps with Flux"},{"content":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!\nNode IP Role Notes molly 192.168.1.51 control plane \u0026ndash;cluster-init daisy 192.168.1.52 control plane joins molly rosie 192.168.1.53 control plane joins molly bessy 192.168.1.54 worker joins any server elsie 192.168.1.55 worker joins any server Secret for all machines You need to supply the same secret to each machine! The initially installed control plane and other control planes or agent nodes need the same secret to join the cluster.\necho \u0026#34;somerandomsecret\u0026#34; \u0026gt; secret Install the initial control plane curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --cluster-init \\ --disable=helm-controller sudo systemctl status k3s.service Additional server installs curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - server \\ --server https://192.168.1.51:6443 \\ --disable=helm-controller Agent install curl -sfL https://get.k3s.io | K3S_TOKEN=$(cat secret) sh -s - agent --server https://192.168.1.51:6443 Connecting to cluster # remote control plane sudo cp /etc/rancher/k3s/k3s.yaml . # On main system scp matt@192.168.1.51:/home/matt/k3s.yaml . vim k3s.yaml # edit IP to point to control plane mkdir ~/.kube mv k3s.yaml .kube/config Uninstalling In case you need to uninstall..\n# servers /usr/local/bin/k3s-uninstall.sh # agents /usr/local/bin/k3s-agent-uninstall.sh Done! üòç\nNext will be setting up Flux for GitOps!\n","date":"10 June, 2025","id":8,"permalink":"/posts/homelab-high-availability-k3s-cluster/","summary":"I\u0026rsquo;ve completed the install of a high availability K3S cluster onto the 5 Optiplex machines. Had one or two issues, that meant I had to reinstall with a new secret, but otherwise was pretty smooth!","tags":"homelab k3s","title":"Homelab high availability K3S cluster install"},{"content":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.\nI will be running a K3S cluster initially and then upgrading to used TalosOS at a later stage. Ill be using GitOps to track the state of the cluster, so the migration to Talos should be easy.I setup the machines with the below network config (static ip addresses)\nvim /etc/netplan/99_config.yaml network: version: 2 renderer: networkd ethernets: eth0: addresses: - 192.168.1.51/24 routes: - to: default via: 192.168.1.1 nameservers: addresses: [1.1.1.1] Machine Names The initial laptop that I ran a test cluster on was one of my daughters, and it had cow stickers all over it, so for better or worse here are my machines and their ips.\nmolly 192.168.1.51 daisy 192.168.1.52 rosie 192.168.1.53 bessy 192.168.1.54 elsie 192.168.1.55 I then applied my changes\u0026hellip;\nsudo netplan apply \u0026hellip;and tested ssh connectivity from my main machine. (I still need to setup dns)\nssh matt@192.168.1.51 We are ready to get the cluster installed and running!\nI crimped some Cat5e connectors too! Not done that in a while üòÄ\n","date":"7 June, 2025","id":9,"permalink":"/posts/homelab-kickoff-host-provisioning/","summary":"To start the homelab I\u0026rsquo;ve completed the installation of Ubuntu Server onto the 5 Dell Optiplex mini-pcs that will be running my Kubernetes homelab.","tags":"homelab k3s","title":"Homelab kickoff - host provisioning"},{"content":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.\nThe layout of the partitions and volumes will look like this:\nThe size of the SSD is 128GB so I\u0026rsquo;ll be arranging the partitions/volumes like below:\ncat /proc/meminfo # how much memory on the system # Memtotal 8010000 kB = 8GB # So swap will be 4GB (50%) # sda is 128GB # par boot 1GB # par swap 4GB # par root 24GB # par home 90GB GPT (GUID Partition Table)or MBR partition table GPT (GUID Partition Table) is a newer partition table schema that allows for unlimited partitions and larger drives and is used with UEFI firmware, which is a newer version of BIOS. The older MBR (Master Boot Record) partition table is legacy format.\nCheck the system boot type We need to verify that the system is using UEFI firmware, which replaces BIOS, and is typically used with GPT partitions.\n# if the below dir exists its an uefi system ls /sys/firmware/efi To determine the current disk partition table we run:\nfdisk -l # youll see Disklabel type: gpt or dos(mbr) # if there is no partition table or you can create a new one fdisk /dev/vda m # man page g #to create a new GPT partition table Partitioning the Disks We will setup two disk partitions that will be the EFI and LVM partitions. Arch wiki suggests boot partition size of 1GB, and rest of the disk will be for LVM (Logical Volume Manager). LVM allows for more flexible management of disks space and our partitions will be setup within this layer.\nlsblk # list block devices (shows current disk layout) fdisk /dev/vda n # add a new partition #default partition number (default): 1 #first sector (default) : 2048 #last sector: +1G #Change the type filesystem type: t # change file system type 1 # select partition L # show partition types # Set partition type to EFI 1 # create data partition n # new partition #default partition number (default): 2 #first sector (default) : 20992000 #last sector (default): 4000797392 t # change file system type 2 # select partition L # show partition types # Set partition type to Linux LVM 44 p # print partition table w # write changes to the disk fdisk -l Encryption \u0026amp; LVM Using LVM makes managing encryption of the disk more straightforward as you only required one key / password to unlock the data.\n# Create the encrypted container on the Linux LVM partition cryptsetup luksFormat /dev/vda2 YES #Enter the passphrase x 2 and open the disk to setup volumes cryptsetup open /dev/vda2 cryptlvm # Create a PV(physical volume) on-top of the opened luks container pvcreate /dev/mapper/cryptlvm # Next create a disk volume group (we are naming ours phoenix) vgcreate phoenix /dev/mapper/cryptlvm # Next create our logical volumes in the volume group lvcreate -L 4G phoenix -n swap lvcreate -L 24G phoenix -n root lvcreate -L 90G phoenix -n home # show our volumes lvdisplay File systems Now that we have or disk arranged how we have planned, we need to setup a file system on each logical volume, we will be using ext4.\n# setup the file system mkfs.ext4 /dev/phoenix/root mkfs.ext4 /dev/phoenix/home # and the swap mkswap /dev/phoenix1/swap # Prepare the boot partition by making a FAT32 file system mkfs.fat -F32 /dev/vda1 df -h # outputs file systems you have (-h is human readable format) Mounting the volumes For our system to access the new partitions / volumes we need to mount them first.\nOrder of mount matters we must map root, boot and then home.\n# mnt will be the install systems root / directory mount /dev/phoenix/root /mnt mount --mkdir /dev/vda1 /mnt/boot mount /dev/phoenix/home /home # activate the swap partition swapon /dev/phoenix/swap df -h And we are done, we can now processed with setting up our Arch Linux system. When you boot the system you will be prompted to enter your configured password, your data is safe!\n","date":"29 May, 2025","id":10,"permalink":"/posts/arch-linux-disk-partitioning-and-encryption/","summary":"Yesterday I set out to work through getting disk encryption configured on one of the 5 OptiPlex 7040 Mini PCS that arrived yesterday. I will be setting up my homelab with these machines and want to make sure I have mastered the process of disk partitioning and encryption.","tags":"arch","title":"Arch Linux disk partitioning and encryption"},{"content":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.\nI\u0026rsquo;ve setup a dotfiles management solution that is the best I\u0026rsquo;ve had. Credit to Mischa and Rio. It is highly portable across bare-metal and containers.\nChezmoi - dot files manager Its a dotfiles manager that manages the state of your .config directory (it does diffs and applies and state changes), across multiple machines.\nYou can read more about it here https://www.chezmoi.io/\nMise - a development environment setup tool With the mise.toml file you specify the dependencies you would like to install for a specific project, it has template support, secret management with sops and age, and more.\nYou can read more about it here https://mise.jdx.dev/\nDevpod I have integrated these tools with devcontainers to build a declarative, reproducible config environment.\nUsing devpods (https://devpod.sh/) and the hooks it provides for executing scripts, I\u0026rsquo;ve combined these three tools into something that I am very happy with.\nAll credit to Mischa and Rio in the KubeCraft community. Thank you so much.\nHyprland on Arch I\u0026rsquo;m not enjoying the desktop experience on Bluefin. Having worked entirely in a window manager for the last few years, just having tmux as a sudo WM is not enough, I need to be able to be virtually 100% keyboard based, so I\u0026rsquo;m busy figuring out next steps. Likely an Arch \u0026amp; Hyprland iteration, with the aim of a setup that is declarative and reproducible.\n","date":"26 May, 2025","id":11,"permalink":"/posts/bluefin-and-dotfiles-management-changes/","summary":"Last week I decided to ditch NixOS, as it was getting in the way of my work. I\u0026rsquo;ve setup a Fedora based distribution called Bluefin, which went smoothly but has taken some getting used to. There are things that have been great, and some not so much, mainly due to the desktop environment, which I\u0026rsquo;ll need to resolve.","tags":"devpod bluefin","title":"Bluefin and dotfiles management changes"},{"content":"18 months on NixOS‚Ä¶ and I‚Äôm done. I‚Äôm moving to Bluefin. I loved the idea of NixOS, a declarative, reproducible, git tracked OS.\nBut‚Ä¶\nIt distracted me from doing real work When I needed flexibility, it refused (or asked for a time investment to get things done) The OS is not relevant in the DevOps community, even though the package manager has its uses. The final straw was not being able to run DevContainers due to a read-only file system (I could have persisted to sort it out, but I\u0026rsquo;m done now). Its over.\nI\u0026rsquo;m grateful for the time I\u0026rsquo;ve had with the ecosystem, it taught me a lot about what I value in a system.\nI\u0026rsquo;ve also come to realise the exorbitant amount of time I\u0026rsquo;ve sunk into \u0026ldquo;just\u0026rdquo; getting my workstation configured, fancy \u0026ldquo;ricing\u0026rdquo; and tweaks every day! I\u0026rsquo;m done now. I need to focus on doing real work.\nI‚Äôm now trying out Bluefin, which is optimised for containers and cloud-native workflows.\nNo regrets. NixOS taught me a lot, but it‚Äôs time for a change. (although Im keeping my github repo still)\nIll be writing about Bluefin in a future post.\nSome Bluefin Kubernetes CLI goodness\u0026hellip;\n","date":"21 May, 2025","id":12,"permalink":"/posts/18-months-on-nixos-and-im-done-moving-to-bluefin/","summary":"18 months on NixOS‚Ä¶ and I‚Äôm done.  I‚Äôm moving to Bluefin. I loved the idea of NixOS,  a declarative, reproducible, git tracked OS.","tags":"NixOS Bluefin","title":"18 Months on NixOs and Im done! Moving to Bluefin."},{"content":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.\nTo install run the following curl -sfL https://get.k3s.io | sh - There is an interesting output to note after the install, K3S installs a single binary and creates 3 symlinks.\n[INFO] Installing k3s to /usr/local/bin/k3s [INFO] Creating /usr/local/bin/kubectl symlink to k3s [INFO] Creating /usr/local/bin/crictl symlink to k3s [INFO] Creating /usr/local/bin/ctr symlink to k3s Check that the cluster is running sudo k3s kubectl get node K9S is a CLI application that manages your Kubernetes cluster To start K9S run the following k9s --kubeconfig /etc/rancher/k3s/k3s.yaml K9S showing currently running pods on the newly installed K3S cluster:\n","date":"17 May, 2025","id":13,"permalink":"/posts/installing-k3s-and-k9s-on-a-ubuntu-server-vm/","summary":"K3S is a lightweight version of Kubernetes and is used for running on resource constrained environments.","tags":"K9S K3S","title":"Installing K3S and K9S on an Ubuntu Server VM"},{"content":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand. My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.\nDocumenting your learning journey through content creation A learning journey, no matter what the material, would benefit from being documented publicly (if appropriate). It serves as a record for you and the world.\nContent doesn\u0026rsquo;t need to be complicated or original\nAs you are going to be posting your content publicly, it will improve the quality of your note taking, and thus your learning and understanding of a topic. The purpose of the content is information transfer not entertainment, there is no need for literary genius or originality.\nThere is no need to be a perfectionist\nShare everything you learn on various platforms, long format and short. If posting to a blog, then reuse your content on LinkedIn and X.\nMake sure to reuse your own content on each platform\nContent generation, rooted in writing, will create opportunity Writing is an enabling and empowering activity. It will allow you to generate a lot of content to share on a variety of platforms. This will open new opportunities, many of which you can not foresee.\nEverything starts with writing, so take personal notes for everything you learn or consume. (Eg in Obsidian)\nBuilding a Personal Brand Being active in the digital world is critical, especially if you are trying to grow your career in an industry that requires proof of experience and skills, the analog world is becoming less relevant.\nIn order to build a strong online presence, you need to have a personal brand that shows your value to others. Your personal brand is how you want others to perceive you, and this is facilitated through writing and associated content creation.\nA strong personal brand will open opportunities as it will have reach and a large audience. If executed consistently you will be able to leverage this to your benefit.\nJobs will come to you and unexpected things will happen.\n","date":"13 May, 2025","id":14,"permalink":"/posts/building-a-personal-brand/","summary":"In this post, I\u0026rsquo;ve tried to outline the purpose of building an online presence and personal brand.  My goals are to grow my brand as a Software and DevOps engineer, but that doesn\u0026rsquo;t preclude this being used for any other type of career, digital or otherwise.","tags":"brand","title":"Building a Personal Brand"},{"content":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.\nMy journey began in networking and support, before I took time out to study and moved into software engineering. Over the years I was naturally drawn into the world of DevOps, where I‚Äôve found a strong sense of alignment with my interests in systems, automation, and continuous learning.\nWhy this blog, and why now? For most of my career, I‚Äôve chosen to stay relatively private. But the industry is changing, and continues to change, faster than ever. With increasing uncertainty and noise, I‚Äôve realised the importance of having a visible, authentic presence and personal brand.\nThis blog is where I document what I‚Äôm learning, reflect on where I‚Äôve been, and share thoughts on the technologies and practices that matter to me.\nI‚Äôve always been driven by curiosity and a desire to improve.\nWelcome and thanks for your interest ü§©\n","date":"12 May, 2025","id":15,"permalink":"/about/","summary":"Howzit, I‚Äôm Matt Britt. I live in Cape Town, South Africa üáøüá¶üòç. I‚Äôve been working in tech for close to 20 years.","tags":"","title":"Start here. This is my personal README.md"},{"content":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.\nWhat is CQRS? CQRS stands for Command Query Responsibility Segregation. Ok, but what is it and how can it help in making your job as a software engineer easier? There are other benefits, like being able to scale the read side separately, but I want to focus on how it benefits you as a developer writing code.\nAt it\u0026rsquo;s core CQRS is about splitting out your systems reads and writes, and having clearly delineated modules in your software that are responsible for each of these elements.\nAs software engineers (at a high level) we take data from a data source, display it to our users in some way, and then save changes back to our data source. Sounds simple but there is complexity in business and domains that can quickly get unwieldy, so any means of reducing the cognitive complexity of any piece of code you are working on, can be very beneficial.\nHalve your contextual understanding requirement Whenever you are working on a piece of code, you need to know the context you are working in, if you can reduce this requirement by half, this makes a huge difference in the effort required to understand what needs to change (and where) and the time it takes to complete the work. You are either working on the read side (components to retrieve and display the data) or the write side (going through the domain to validate your business invariant\u0026rsquo;s)\nIf you had to embrace CQRS completely, you would separate data sources for the read and write side, synced by streams, events or etl. But you can gain the benefits in your code without these complexities. Arranging your project by logical read and write data, still using a single physical data source, is a great start.\nYou can gain the benefits of CQRS without the infrastructural complexities associated with it (eg read and write DBs)\nThis single pattern will make a huge difference in how code gets organise, both in your editor, and in your head.\n","date":"27 January, 2020","id":16,"permalink":"/posts/cqrs-how-it-can-simplify-your-software-and-your-job/","summary":"CQRS can help you break up your solutions into logical parts that reduce the contextual burden, making your software more understandable and your job easier.","tags":"Design Architecture DDD","title":"CQRS - how it can simplify your software and your job"},{"content":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.\nIf you\u0026rsquo;re a professional in DevOps or software engineering, Vim should not be optional.\nThat doesn‚Äôt mean you need to live inside vim or switch to Neovim overnight. Just start with Vim motions, they‚Äôre built into most editors via extensions.\nYou don‚Äôt need ‚Äúfull Vim‚Äù. Use VSCode with the Vim plugin. That‚Äôs more than enough to get the benefits.\nHere‚Äôs why you should start learning it now:\nYou keep your hands on the keyboard, which means less context switching and fewer mouse detours\nYou build deep muscle memory for editing, searching, and refactoring\nFor DevOps and system administration, it‚Äôs essential\nThe learning curve can be steep. But it doesn‚Äôt have to be. Start with just a few motions. Use them until they‚Äôre second nature. Then add more.\nVim is a lifelong multiplier. There‚Äôs no excuse not to start.\n","date":"25 April, 2019","id":17,"permalink":"/posts/why-every-engineer-should-learn-vim/","summary":"I\u0026rsquo;ve been using vim for close to 10 years, in various forms, vim, nvim, vscode extensions.","tags":"","title":"Why every engineer should learn vim"},{"content":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.\nWhenever I made an effort to touch type, my eyes inevitably always drifted down. The keyboard was there, and the temptation was too strong.\nI decided to switch to Dvorak and at the same time I bought a Kinesis Advantage 2.\nDvorak is an alternative keyboard layout which places the most commonly used letters under your strongest fingers and minimises finger movement.\nI wasn\u0026rsquo;t that bothered about comparing to other layouts like Colmak. The Kinesis has Dvorak built in so it made sense.\nBecause I didn‚Äôt know the layout, and more importantly, the keys didn‚Äôt match the labels on the board. Looking down was useless.\nIt was tough! Painfully slow, but I wasn\u0026rsquo;t able to cheat by looking at the keyboard, so I just slogged through and slowly my speeds went up and now I\u0026rsquo;m fully productive!\nThe result? I finally learned to touch type after more than a decade in this career.\nNot just that, it rewired how I work:\nLess cognitive load while coding Less fatigue from hunting keys More confidence in every terminal session Looked more professional! This was one of the best decisions I have made and its be so worthwhile!\n","date":"25 March, 2019","id":18,"permalink":"/posts/dvorak-how-touch-typing-changed-how-i-work/","summary":"After 14 years as a software developer, I still wasn‚Äôt a true touch typist, I would hen-peck my way through my work, I managed but it was not optimal.","tags":"","title":"Dvorak - how touch typing changed how I work"}]